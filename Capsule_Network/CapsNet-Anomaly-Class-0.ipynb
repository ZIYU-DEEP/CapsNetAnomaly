{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/inderjeet78/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "from __future__ import division, print_function, unicode_literals\n",
    "\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "tf.reset_default_graph()\n",
    "\n",
    "np.random.seed(42)\n",
    "tf.set_random_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-2-4141630e56b4>:3: read_data_sets (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n",
      "WARNING:tensorflow:From /home/inderjeet78/anaconda3/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:260: maybe_download (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please write your own downloading logic.\n",
      "WARNING:tensorflow:From /home/inderjeet78/anaconda3/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:262: extract_images (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting /tmp/data/train-images-idx3-ubyte.gz\n",
      "WARNING:tensorflow:From /home/inderjeet78/anaconda3/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:267: extract_labels (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting /tmp/data/train-labels-idx1-ubyte.gz\n",
      "Extracting /tmp/data/t10k-images-idx3-ubyte.gz\n",
      "Extracting /tmp/data/t10k-labels-idx1-ubyte.gz\n",
      "WARNING:tensorflow:From /home/inderjeet78/anaconda3/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:290: DataSet.__init__ (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "\n",
    "mnist = input_data.read_data_sets(\"/tmp/data/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training data prepration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5444, 785)\n",
      "(49556, 785)\n",
      "[[0. 0. 0. ... 0. 0. 7.]\n",
      " [0. 0. 0. ... 0. 0. 3.]\n",
      " [0. 0. 0. ... 0. 0. 4.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 5.]\n",
      " [0. 0. 0. ... 0. 0. 6.]\n",
      " [0. 0. 0. ... 0. 0. 8.]]\n",
      "(49556, 785)\n",
      "(321, 785)\n"
     ]
    }
   ],
   "source": [
    "c1_x = mnist.train.images[mnist.train.labels==0]\n",
    "c1_y = mnist.train.labels[mnist.train.labels==0]\n",
    "c1_y = c1_y[:,None]\n",
    "other_x = mnist.train.images[mnist.train.labels!=0]\n",
    "other_y = mnist.train.labels[mnist.train.labels!=0]\n",
    "other_y=other_y[:,None]\n",
    "\n",
    "np.random.seed(42)\n",
    "c1 = np.concatenate((c1_x,c1_y),axis=1)\n",
    "others = np.concatenate((other_x,other_y), axis=1)\n",
    "print(c1.shape)\n",
    "print(others.shape)\n",
    "print(others)\n",
    "np.random.shuffle(others)\n",
    "others = np.array(others)\n",
    "print(others.shape)\n",
    "others321 = others[0:321,:]\n",
    "print(others321.shape)\n",
    "train = np.concatenate((c1,others321),axis=0)\n",
    "np.random.shuffle(train)\n",
    "X_train = train[:,0:-1]\n",
    "Y_train = train[:,-1]\n",
    "Y_train[Y_train!=0]=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "321.0"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(Y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Validation data prepration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "valX_ones = mnist.validation.images[mnist.validation.labels==0]\n",
    "valY_ones = mnist.validation.labels[mnist.validation.labels==0]\n",
    "valX_others = mnist.validation.images[mnist.validation.labels!=0]\n",
    "valY_others = mnist.validation.labels[mnist.validation.labels!=0]\n",
    "valY_ones = valY_ones[:,None]\n",
    "valY_others = valY_others[:,None]\n",
    "val_ones = np.concatenate((valX_ones,valY_ones),axis=1)\n",
    "val_others = np.concatenate((valX_others,valY_others),axis=1)\n",
    "np.random.shuffle(val_others)\n",
    "val_others137 = val_others[0:137,:]\n",
    "val = np.concatenate((val_ones,val_others137),axis=0)\n",
    "np.random.shuffle(val)\n",
    "valX = val[:,0:-1]\n",
    "valY = val[:,-1]\n",
    "valY[valY!=0]=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "137.0"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(valY)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test data prepration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "testX_ones = mnist.test.images[mnist.test.labels==0]\n",
    "testY_ones = mnist.test.labels[mnist.test.labels==0]\n",
    "testX_others = mnist.test.images[mnist.test.labels!=0]\n",
    "testY_others = mnist.test.labels[mnist.test.labels!=0]\n",
    "testY_ones = testY_ones[:,None]\n",
    "testY_others = testY_others[:,None]\n",
    "test_ones = np.concatenate((testX_ones,testY_ones),axis=1)\n",
    "test_others = np.concatenate((testX_others,testY_others),axis=1)\n",
    "np.random.shuffle(test_others)\n",
    "test_others137 = test_others[0:137,:]\n",
    "test = np.concatenate((test_ones,test_others137),axis=0)\n",
    "np.random.shuffle(test)\n",
    "testX = test[:,0:-1]\n",
    "testY = test[:,-1]\n",
    "testY[testY!=0]=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "137.0"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(testY)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data checking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAABOYAAACACAYAAACvHmZ+AAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMi4yLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvhp/UCwAAIABJREFUeJzt3Xm8TfX+x/GvTCkOSVISEU26jRINaDKkIuSikiEpDSoqpRIKIZpINxmrB5WpJFFJo1RSXWT4CRVCSJwQ5/dHt0+f77ezV2uvvfZee+/zev71XndN386y1t573fX5rEJ5eXkGAAAAAAAAQGodEPUAAAAAAAAAgIKIG3MAAAAAAABABLgxBwAAAAAAAESAG3MAAAAAAABABLgxBwAAAAAAAESAG3MAAAAAAABABLgxBwAAAAAAAESAG3MAAAAAAABABLgxBwAAAAAAAESgSIr3l5fi/eEPhULaDscvGmEdP2M4hlHhHMxsHL/MxjU083EOZjaOX2bjGpr5OAczG8cvs/k6fjwxBwAAAAAAAESAG3MAAAAAAABABLgxBwAAAAAAAESAG3MAAAAAAABABLgxBwAAAAAAAESAG3MAAAAAAABABLgxBwAAAAAAAESAG3MAAAAAAABABLgxBwAAAAAAAESAG3MAAAAAAABABLgxBwAAAAAAAESAG3MAAAAAAABABIpEPYCo1apVS/Jnn31mzTvgAH/3Lffv3y+5ZcuWkkeOHGktV65cuSBDBGCM2bFjh+Rbb73Vmjdu3DjJjz/+uORbbrkl+QND3GbPnm1NP/PMM5KnTZvmaxv9+vWT3Lp1a2te9erVExhdwTV27Fhrev369ZI//fRTyY0bN7aWa9iwoeTKlSsnZ3BIma1bt0oePHiwNU9/32natKnkE0880VouJydHcpEiBf6rZuj27t1rTY8aNUpy3759JW/atMlabtWqVZKrVq2apNHhT/PmzZPcp08fyUWLFrWWu/HGGyXr3yIbNmywlps7d65kfZyvu+66BEeKRK1YsULysGHDJLu/BfXn5xtvvJH8gWWJBQsWWNPTp0+XPHDgwIS3n5eXJ/maa66RPHz4cGu5smXLJrwvIBaemAMAAAAAAAAiwI05AAAAAAAAIALcmAMAAAAAAAAiUEjXVKdASncWy/z58yXrvgxr1qyxlitcuLCv7e3bty/fddq2bWstV7duXcldunTxte2QFAppO2lx/IJYt26dZLffzRFHHJHq4cQrrONnTAYfwzFjxkju3LlzzOV03xy3F12EPecK/Dmor7vNmjWz5m3bti2hbR9//PHW9JIlSxLaXj4y+vh99dVX1rTux7Jy5UrJn3/+ubWc7ifmpUaNGpL1516vXr2s5YoVK+Zre0nANdShe18ZY8yLL76Yb961a1eg7T/00EOS77///kDbcGT0ORjU+++/L7lChQqSGzRoYC33448/+treKaecIllfk0uVKhV0iH4ViOOne8AZY3/W7dy5M9R9ValSRXLPnj2tefo3Rkg9HrmGOr777jtrWv/N3X8Hmu7D+t5770k++uijwxtc/jL6HDz88MOt6Z9//jnU7cf6La/7zRljzDnnnCO5U6dOoY7hH2T08Qub25/xwQcflOy+M0DTPanLly8v+eabb7aW099rQ+Lr+PHEHAAAAAAAABABbswBAAAAAAAAESgQpaz6cX1jjOnWrZvkb7/9VrJ+jNWYxEtZvTzwwAOSe/fu7WudBBTIx191eZwudXPLqd555x3JxYsXl/zJJ59Yy9WpU0fyUUcdFdo4fSiwJQS63GrEiBGSX3311UDbGzRokOQePXoEHlcABfIc1GbMmCHZLWVN1IEHHmhNBy2/85DRx8/9++zevTsl+x0wYIA1fc8996Rkv/kosNfQ3Nxcybq8dPTo0dZyW7ZsCXW/FStWlOy27tAlznGU2WX0ORiGsK+hY8eOlXzttdcmvL1/kFXHT5elvv3225LdsrdffvklZWPSVqxYIfnYY48NY5MF9hqq6ePepEkTa54uO9dycnKs6e3bt0vW18ZRo0aFMUQvGXcO9u/fX/Ijjzxizdu7d2+o+/L7W75kyZKS+/XrJ9kthUyCjDt+YdDXsq5du0p27+38/vvvCe1Hl7UaY//mbNGiRULb/h9KWQEAAAAAAIB0xY05AAAAAAAAIAJZW8q6dOlSyZdeeqk17/vvv893nVSWsp566qmSP/30U1/rJCBrH3/Vj67qt3YaY79t8IknnpB87rnnWsvdfvvtkseNGyd5ypQp1nJ9+/aV7L5tMMmyuoRAv02uffv21jxdGnDAAX/9/wjusT7ooIMkDx48WPKHH35oLafLpvQj6HfddVe8w45X1p6D2q+//mpNT506VfIdd9whOeyyOVf37t0lP/bYY2FsMqOP3/PPP29N+32T2MUXXyw5yNur9fE3xpiOHTtK1iVfZ5xxRtzbjlNWX0M19011utwqyHeNhg0bWtO1a9eWrEv43GutF33+H3LIIX5Xy+hzMKhFixZJvuyyyyT7fQurl5o1a0r++OOPrXkHH3xwwtt3ZNXx099B9HXNL/0ZZYz9pkfN/R46c+ZMyV5lsnfffbdk/V3HGGOKFi3qe5xKgbmGetHHTf+ucJ133nmS9VsgjbHbKZ199tmS3XMwCTLuHGzevLnk119/Pan7CvJbXpc46lYRxhhzzDHHSA7pjfQZd/yCcN923KZNG8lui6lYatWqJfmqq66KudwLL7wg+csvv7TmVa9eXfLy5ct97fcfUMoKAAAAAAAApCtuzAEAAAAAAAAR4MYcAAAAAAAAEIGM7jG3Zs0ayY0bN7bm6R5zujeVl/3791vTQdYLss6NN95ozdOv6A1J1tal694euj+cS79mW/feMMaYb775RvIpp5wi2X118vr16wOPM0FZ19vjp59+ktyoUSPJixcvjrmO7hU5Y8aMmMvl5uZKHjRokDVP9wnUPR9mz55tLVevXr2Y2w8oa89B3cPK7W+Vgv6Z+apYsaLkSZMmWfPq1q0bZJMZffzGjx9vTbu9HP+Uk5NjTc+aNUtykL+b2wtywIABkvV56vZR0v1BQpJ111BNf4YNHTrUmuf1uajpfjgTJ06UrHsgGWNMoUJ//Sn1MdT9CI0x5qOPPoq5r/nz50t2e756yOhz0K9Vq1ZZ0/Xr15f8ww8/JG2/mzdvtqbLli0b9i4y+vjpa6ExxrRr107y1q1bfW1D95H74IMPAo2jS5cukv/zn//4Wsf9HA54fc3qa6iX1atXS9a949w+j/papr+jur3RdH9VeszZvvrqK2u6Q4cOMeeFLUiPOS9LliyRrPuVJSDtj19QCxYskKx/ExpjzLZt2/Jdp1KlSta0vibfeuutkr36I+vfnGeddZY1r0SJEvkuZ4wxlStXjrlND/SYAwAAAAAAANIVN+YAAAAAAACACBSJegCJGDhwoOSVK1da83RJadBHUoOsF8bjr/Bv+vTpMefpUgy3XFibPHlyqGPC323YsMGavuiiiyTrx71dDzzwgOQ+ffr42pd+/NhdZ8iQIZJ37dolee7cudZySShlzSqvvfaa5HfffVfy7t27oxjO3+iSrzvvvNOal4JykbTzyiuv+FrOLSEIWPYrdBmKMfZr71u1aiW5RYsW1nJTp06VfMYZZyQ0hmz19ddfS77wwgsluyWJsVSpUsWafvPNNyX7LbvR19qePXta85o3bx5zvZkzZ0qOo5Q1a/3222+SH3/8cWue3/LVM888U3Lt2rUlP/300zHXKV26tGS+u/7dL7/8Ill/FzHGf/mqbp3y8MMPJzwmXaKlr+te49Gl6cYkpVVAVtmzZ4813bp1a8n6fDzppJOs5fRv0jJlyiRpdNnNbSuT7PLVZNKtO55//vkIR5Ketm/fLlm3I4tVumqM/X1h5MiR1jxdsnrooYf6GoNuX1WjRg1rnm4R4rYN6N+/v6/tB8ETcwAAAAAAAEAEuDEHAAAAAAAARIAbcwAAAAAAAEAE0r7H3Jo1a6xpXcMf9HXj6cb979DT9F/5+6vedf8v3ZulUCH7TcS6d1lOTk7M7ffr1y/fbei+EkhMr169rOmlS5dK1n9ztwdL9+7dQx2H7hOjx6T7pBljTG5urmTdR6mgcv8+zz33nOQgfeUOPPBAa/ryyy+XfMIJJ0i+/fbbreV0/5HOnTtL3rFjR9xjyHZffPGFZN0/zKV7bLg9O8Kmj/v5558v+fXXX7eW09cHesz9YfHixdb0xRdfLNlvX7kePXpI7tu3rzXPPSfjVapUKd/LDhs2TLLuw1OQ6O8uumfnU089FWh7uvdYkSJ/fbX36jFXvnx5yfSY+zt9Xfrss898rXPPPfdY0/o8C+NvXLNmTcktW7aU7PZAQnCPPvqoNa2Pve4dN2bMGGs5evcFs3fvXsk7d+5MeHu6x7wx9vfLRx55JOZ6up9dp06dEh7TrFmzAq2XrfLy8qxp3fvdq0dm8eLFJT/22GOS9bUwDE8++aQ13aBBA8kLFy4MdV9eeGIOAAAAAAAAiAA35gAAAAAAAIAIpH0pq1uiMXr06KTtS5dQGWPMhAkTfK2nX1MfxLJly2JOU8pqzNq1a63pokWLStZldPXr17eW8/u6cl1KqfOkSZOs5YYPH+5re/jDkCFDJI8bN86ap//OLVq0kNytWzdruUTLq1zXXnutZF3K+vHHH1vLrVy5UvLJJ58c6hgyhf73rx85N8b7deax1K1bV7JbTqDLGr20atVK8v333y+ZUlZjVq1aZU03a9ZMsi4VMca+ht50002Sy5Ytm6TRxUeXK1x99dURjiRan3/+ueQmTZpY8/yWr+pjrctfw762xlPuU6xYsVD3nQl06aoxxtx9992S3RKaWHRLjv79+1vzmjZtKtmrdF2rVq2a5IJ4TFzfffedNX3DDTf4Wk9fa932C/r8C1vz5s0lU8qamBdffFGyV7mjLr2ndDUcCxYskOy2swnC/R7z8ssv+1qvevXqknU5rG4TYIwxGzZsSGB0BZfbDuCll17Kd7kKFSpY07qVzvHHHx/+wP7n8MMPjzlP/xtNNp6YAwAAAAAAACLAjTkAAAAAAAAgAmlfyuqWibpvW4ll//79vpbTj1aedtpp/gem1KhRQ/Ly5ct9reM1Pv34vPtGuqBjzDT6TVbuW9v27Nkj+eyzz5Y8bdo0azldQqDX0W919TJ//nx/g0W+hg4dGnOePmd0eXo8b/ZDuPSbAY0x5pprrpH8+++/B9qmLp3Tb1H1W7rqev/99yVv2bIl0Day1a+//mpNr1u3Luayd955p+QuXbokbUxedJuGcuXKWfO+/vprye4bW3XJXrZx30B+xRVXSN60aZOvbbilc7rkW5+PYRsxYoTvZWfOnJm0caQT3WrjrrvusuYFefuqfptthw4dYi73ww8/+NqefhPevn374h5PtnG/s7jXVO2CCy6QPHnyZMnJLF11pfJNgdlIt8nRJchu2bl+c3mQ1kUrVqyIOS+ZpXkFVdeuXRPehi4THz9+vDXP/U4Si27v4Lbzad++fQKjyxy6LZBuJeTF/VsVtHOEJ+YAAAAAAACACHBjDgAAAAAAAIgAN+YAAAAAAACACKR9jzm3p1zhwoXj3kaQdeIxceJEyXXq1Il7/WSPLxP89NNP1vTw4cMl6/5wrlGjRknOycmJudz27dsl//jjj77GVKZMGV/L4Q/33HOPNe3VA0z380hlXzn3dd0FnT4vnn32WWtekL5ybv8v/Tr0gw8+OO7tufQry/32mNP9YfCHJk2aRD0E67NyzJgx1rzLLrtM8ltvvWXNy7Yec7m5uZLvvvtua97GjRvj3p7uUWuMMb179w42MB90Dy7dr8x1+OGHW9Onnnpq0saUTtavXy85SE85Y+xzoVmzZr7WGTt2rK/ldA/jEiVKxDWubDFlyhTJ+vvkP2nQoIHkVPaV0+bMmeNrubp16yZ5JJlJf5boHp7ubwn9GXTYYYfFvR+vXoBuH3Ekrm3btlEPwRhjf7ZPnz7dmldQesy1bNlS8rJly2Iud/3110tOZi/cTMATcwAAAAAAAEAEuDEHAAAAAAAARCDtS1mToV27dpIrVaoU4UjwJ/f11rrEztW/f3/JJ598sq/tr169WrJbNqv16NFD8iGHHOJr265p06ZJXrVqlTVPPxJ/3333ST7//PMD7Stq+r/n0UcfjbncVVddZU136dIlaWPyosuCChUqJPmSSy6xlvP77yoT7dq1S3L37t0lz5w5M9D2Lr30UsmjR4+25gUpX929e7fkgQMHWvNefvllX9vQx9nr3yXSQyrL2dOBbs/QrVs3ye+9917C265atWrC2/Ciy2sbNWokefHixTHXueOOO6zpbD3e+tplzN+vX36cdNJJ1rQuS/X6TjJ//nzJ3333Xdz7LajeffddyXv37o253HnnnWdNJ7NE3Eu/fv0kf/DBB77WycvLS9ZwMor7/eTrr7+WXLJkScmTJk2ylgtSvqq3PWvWrJjLHXHEEXFvO5vt27cv4W2E/e/d3V4YY8xWbsuGpUuX+lpPl9vr32ap5PUdJpWtrXhiDgAAAAAAAIgAN+YAAAAAAACACBTIUtZzzjlHcrly5SIcScH2/vvvS9bln64aNWpY0/qtc998803M9Q499FDJS5YskeyWm+jHlAcPHizZLSv6+eefJa9cuTLf9Y3x/xiuLq9dvny5r3XSjS4L9vrvbty4cSqG8zdu2bIeo87ly5dP2Zii9sILL0geN25coG3otybp8tIDDzww+MD+R7+Z8oknngi0jSpVqkj2elszEAX9NlO/b9A85phjrOl69epJ/vLLLyV/+umniQ3O4Zb33XbbbZK9Sj/uvPPOfNfJZvr7gzF/f9N1LKVLl5asW3UY47+lxpAhQyTv3LnT1zoF1Y4dOyT7/Vvp3w2p5J5/uvTWL/dztEKFCpLr168faFyZYvPmzZJvvvlma57+DtixY0fJDRs2THi/8+bNy3c/xhhz4oknStatQGBM4cKFE95GGKWQCxYskPx///d/1rwgY3TbGunP6bPOOivu7aWT3377TfLDDz9szdNtO9zvMLrVzbBhwyRfe+211nIHHJCaZ8i8WuXo7zPJxhNzAAAAAAAAQAS4MQcAAAAAAABEgBtzAAAAAAAAQATSvsec2y/Fby32/v37Y85bu3at5F27dkk+6KCD4hzdH84880zJfmuhvcZXUDzyyCOSvXoCuH3CWrZsKdmrj0PFihUl//DDD772pectXLjQ13KnnXaaNa969eox17vkkkskt27dOuZymWLixIm+lmvfvn2SR/KXH3/8UXLTpk1jLleqVCnJ3bt3T+qYMl21atWs6UT7yi1atMia1r3uRowYEff2ihcvbk336dMn7m0AqfLUU0/5Wq5y5cqS33zzTWue/pzRPevC7scydOhQa3ry5Mn5Lnf00Udb07fccovkYsWKhTqmdKU/e+Kh+0xdccUVvtbZsGGDNf3f//437v127tw57nWyge5h5NXfWHN7QR533HGSr7vuujCGlS+3j7LfHnPnnXeeZLf3Ye3atRMfWJrat2+fNd2hQwfJuheWMcacdNJJkh988MGE9637aXn1q6pZs6bkMPryIhxLly6VfP311+f7vwel+5wbY/dYz/QecxMmTJDsfi4VKfLXbaZXXnnFmqd7XerrlXt/JJk95vRxeeONN6x5+jdiKvuk88QcAAAAAAAAEAFuzAEAAAAAAAARSPtS1nLlylnT9erVk/zBBx/42ob7auOBAwdKbt68uWS3JDGWKVOmWNP6Mcsgr1F21zn33HMlu//92WTr1q2+ltu+fbs1/d577/lab/369ZJ1qdvu3btjrjN16lTJderU8bWfkiVLWtMlSpTwtV6m0sdN/41dqSxf1XSZ2OLFi2Mup8sdTz/99GQOKeN169bNms7JyfG1Xm5urmTdlqBdu3bWckFLwP504403WtOHHHJIQtvLdrqFQzrQ/05c2Xg9dctSYznhhBMke7VIcD+Dgti7d69kXb7qVeKlS23feusta55b2pqttm3bJnnSpEm+1ytdurTk2267zdc6ukzoyiuvtOatXr3a1zZ0W4IqVar4Wifb6HJsXTo8a9asmOt06tTJmvZqk5Eo/b3qrrvuCrQN/Tsim0tXXW7rm5kzZ8ZcdtiwYZLLli2b8L6fffZZyfr3qdtqw28rAwTTsGFDa/ree++VfN9998VcT5ci6/YQYXBLIW+66aZQtx8lt92FdsEFF0j2+p2ly1yD3EeJhy6VffLJJyW7pe7ly5eXfOyxxyZ1TBpPzAEAAAAAAAAR4MYcAAAAAAAAEIG0L2XVpRLG2G/s04+C+i1rjcf8+fMlv/TSS5L9vsUpqLZt20quVKlSUvcVpTZt2kiO5424J598smT9JheXLjHWj467pbA1atSQrP+9HXbYYb7HVJDotxq7bwzTUvn3GzJkiOTHHnss5nJNmjSR3LVr16SOKZusXLnSmtZ/by+zZ8+W/Pbbb4c6Jn1tvPrqq0PddrbTpXPLli2LZAy6tFm/JdEY+011gwYNStmYUuXjjz+W7PWW8IsuuihpY9i4caM1rf9NxHrzqjGxy1e9Sm2zmX4LpN/2HMYYM2DAAMm1atWKudycOXMk67LiTz75xPe+NP22yDJlygTaRqbT3w39lsq7v0USbTPjvj1Un0u63Y7+HRKPjh07BhtYhnPf/KhVrFjRmvbbriaWhQsXWtNjxozJd7lWrVpZ09ncoigdrFu3zpp2W53Eos/JsMsp9dtJjcmuFh1eLY169Ojhaxt+W4mFYfz48ZKfeeYZyW6LHr1cKvHEHAAAAAAAABABbswBAAAAAAAAEeDGHAAAAAAAABCBtO8x5zr++OMlH3fccZKD9pjTvW10XxtjjNmyZYvk77//PtD2/WjevLk13axZs6TtK53onjY6h2XHjh2Se/XqFXM5/TrnU045JfRxZBv9N9I9kHQvHGOMmTdvXqj71f0flixZYs17+umnJetXnuvXXRtjzOuvvx7qmAoK3dszSrqvnL5uer2GvaDQn43G2P2o+vbta81bvXq15NGjR0vu1KlTqGP6+eefrelYfeU2bdpkLee3h2G2C7vvyuDBgyW7f2P3GPzJ7a1FX7lwlC1bVrL+7Pzoo4+s5XSPxd9++y3u/bjHKJv7Fvu1a9cuyVOmTPG1Ts+ePa3pDz/8UHLr1q3jHoN7TQ7SM1B/v2nXrp01z+2nls127twp2f2ukpeXJ7lhw4bWvJIlS8a9L32d7Ny5szXvq6++klylShXJt956a9z7QebT/75uvvnmCEcSvsWLF0v2+lyK6vNm//79km+44QZr3tixYyXrPqu33367tVy9evWSM7h/wBNzAAAAAAAAQAS4MQcAAAAAAABEoJB+zDcFkraza665xpqeOHGiZP1qdC/60ceg6/ldR5deua/cToJCIW0npf9YwqTLFVq2bGnNK168uGT9avpatWolf2D+hHX8jAn5GDZq1EiyW8qqvfbaa9Z0kyZN8l1u69at1vTatWslz549W7JXafK5556b7zrG/L1cPYXS5hxcsWKF5Msvv1zyt99+m+imQ1e6dGlr+u2335ac4vLVtDl+QbglO7r0R/8d586day2nXx9fuHBhyV6lC7pcfOTIkda8d955J9919DXYGGPGjBkjuU2bNjH3FYe0uobq0reXX3455nJdu3aV7FVOvmbNGsnuNVRfKxctWiT5p59+irm97t27S+7WrZs1r1q1ajHXS7K0PAd1y5PDDjvM93r62qbPp927dyc8Jl2++uabb1rzjjnmmIS3H1DaHL/c3FzJujWH/mxMV/rfmC698vpOFJK0uoZq+rjpFkeupUuXWtNey2qbN2+WrMtXZ8yYYS2ny/aGDx8u2W1XFKG0OQc1fT4+9NBD1ryhQ4eGuStPul2O/r4TlP6sXLZsWcLbM2l0/PRvOv07wqXPObfdStjWrVsn+ZZbbpE8ffp0azldvqq/6+j2L0ni6/jxxBwAAAAAAAAQAW7MAQAAAAAAABHIuLeyxjJhwgRrWpcGuI8x+hXkUVa/61x55ZVxbxvB6X8PhQrZT5MWKfLXaVC0aNGUjSkbtGjRQrJXKesdd9xhTT/xxBP5Lrd+/Xpr+ptvvpGsy+7dY3jGGWdI1o/CR1i6mrZ0mZMuXWzatKm1nH7rUirpt0BOnTrVmnfqqaemejhZwS2DbN++veQvvvhCsn5TpDF2GaMu+7nvvvus5fQbsP3Sbxx1y+3ctylnG/12QK9S1pdeekmyfpOkMXbZhi7l2rt3r7Xcxo0b8932kUceaU0/99xzki+88ELJfCZ6058x7lvdva6h27dvD3Uc+q2c+o2fEZaupq0SJUpI1u040rGUtUGDBtZ0nz59JJ9//vkpHk1m81u6unLlSmtatx7Qn5f635ExxvTo0UNyGpWvpj39d6xdu7Y1r0KFCpI3bNiQsjH5dfTRR0t2r/9PPfVUqoeTMro9im7h5bYE++WXX0Ldrz43BwwYYM3TbVR0qw7dksUYu8xcfxdOFzwxBwAAAAAAAESAG3MAAAAAAABABLgxBwAAAAAAAESgkO7blAIp29nnn38uuU6dOr7W0a9KNsZ/v7hYr1jWPT+MMeacc86RrF+5nQJp84rlqOzZs0dy/fr1rXmffPKJ5DPPPFPyu+++ay138MEHJ2dw/yxtX1Ove0u5tfq6t6N7nXF7xPlx+umnS9a9PIwxpkmTJpJLlSoV97ZTIO3PwSFDhljTo0aNkrxq1apQ91WsWDFrunfv3pL1dTON+iOl/fHzsnXrVmu6S5cukl955ZWk7bd06dLWtO4r9+KLL0o+4ogjkjaG/0mra6j+zvDoo49Kdnv3haFMmTKSr7rqKsnudxD92Zem0v4cvOmmm6zpZ555JtTtV6lSRfK9995rzevQoYPkIP2RUyAtj5/ua6t7KxpjzNKlS8Pclad69erlm3v16mUtF2Hf3LS6hmqbNm2SfNZZZ1nz1qxZI3ny5MnWvC1btkh+9dVXJS9YsMBaTn/PrVGjhmS3x1UG9JVLy3PQi+4zfN1111nz3L6riYr1W95Lx44dJevvzEmSlsdP98GcN2+eNU/3CNSfUcbY3zmMtCT0AAAD7UlEQVR2794t+fvvv7eWmzJliuRFixblu44x9m8/3Y+6X79+1nL6+ppivo4fT8wBAAAAAAAAEeDGHAAAAAAAABCBrC1l1Y+4fvvtt9Y8/fjxtGnTJActZdWPNk+YMEFypUqVrOXKlSvna3tJkJaPv6aLCy64QHKzZs0k33DDDdZyxYsXT9mYHGlbQqDl5uZa04MGDZK8efNma97IkSMlt2rVSnLlypWt5fSjzrpcNcKy4qAy7hxcsWKF5Pnz58dc7rbbbpPsVVqgXyXfs2dPa17btm2DDDGVMu74edm2bZtkXcqqW0AY478UT5es6pLoihUrWss1btw4rnGGKG2vofp7x/jx4615y5cvj3t7+vPMGGNq164tOScnJ+7tpZG0Pwfd75D6eHbq1MnXNv79739b09WrV5es20VUrVo1yBCjlPbHzy2N0qWPc+bMsebp7/p+6c+9iy66yJqny8GKFi0a97ZTIG2vodprr71mTV9xxRV/7TRgSxV97j700EOSjzzyyCBDjFLan4NeFi5caE3PmDFD8sCBAxPefs2aNSWPGDHC1zrly5eXXK1atYTH8A/S8vjp75P6e74xxqxduzbMXVml/G47hzZt2kg+9thjQ91vSChlBQAAAAAAANIVN+YAAAAAAACACHBjDgAAAAAAAIhA1vaYgyUt69LhW0b09oAnzsHMxvHLbFxDMx/nYGbj+GU2rqGZj3Mws6X98du+fbs13atXL8kbN2605k2ZMkVyvXr1JB911FHWcldeeaXkf/3rX5LTtI+cF3rMAQAAAAAAAOmKG3MAAAAAAABABChlLRjS/vFXeKKEIPNxDmY2jl9m4xqa+TgHMxvHL7NxDc18nIOZjeOX2ShlBQAAAAAAANIVN+YAAAAAAACACHBjDgAAAAAAAIgAN+YAAAAAAACACHBjDgAAAAAAAIgAN+YAAAAAAACACHBjDgAAAAAAAIgAN+YAAAAAAACACHBjDgAAAAAAAIhAoby8vKjHAAAAAAAAABQ4PDEHAAAAAAAARIAbcwAAAAAAAEAEuDEHAAAAAAAARIAbcwAAAAAAAEAEuDEHAAAAAAAARIAbcwAAAAAAAEAEuDEHAAAAAAAARIAbcwAAAAAAAEAEuDEHAAAAAAAARIAbcwAAAAAAAEAEuDEHAAAAAAAARIAbcwAAAAAAAEAEuDEHAAAAAAAARIAbcwAAAAAAAEAEuDEHAAAAAAAARIAbcwAAAAAAAEAEuDEHAAAAAAAARIAbcwAAAAAAAEAEuDEHAAAAAAAARIAbcwAAAAAAAEAEuDEHAAAAAAAARIAbcwAAAAAAAEAEuDEHAAAAAAAAROD/AY4nVrpzfR6tAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 3600x216 with 11 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "n_samples = 25\n",
    "\n",
    "plt.figure(figsize=(n_samples * 2, 3))\n",
    "for index in range(14,n_samples):\n",
    "    plt.subplot(1, n_samples, index + 1)\n",
    "    sample_image = X_train[index].reshape(28, 28)\n",
    "    plt.imshow(sample_image, cmap=\"binary\")\n",
    "    plt.axis(\"off\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0.], dtype=float32)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_train[:n_samples]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparing Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-11-d9e33cb66a21>:32: calling reduce_sum (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "keep_dims is deprecated, use keepdims instead\n",
      "WARNING:tensorflow:From <ipython-input-11-d9e33cb66a21>:67: calling softmax (from tensorflow.python.ops.nn_ops) with dim is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "dim is deprecated, use axis instead\n"
     ]
    }
   ],
   "source": [
    "X = tf.placeholder(shape=[None, 28, 28, 1], dtype=tf.float32, name=\"X\")\n",
    "\n",
    "caps1_n_maps = 32\n",
    "caps1_n_caps = caps1_n_maps * 6 * 6  # 1152 primary capsules\n",
    "caps1_n_dims = 8\n",
    "\n",
    "conv1_params = {\n",
    "    \"filters\": 256,\n",
    "    \"kernel_size\": 9,\n",
    "    \"strides\": 1,\n",
    "    \"padding\": \"valid\",\n",
    "    \"activation\": tf.nn.relu,\n",
    "}\n",
    "\n",
    "conv2_params = {\n",
    "    \"filters\": caps1_n_maps * caps1_n_dims, # 256 convolutional filters\n",
    "    \"kernel_size\": 9,\n",
    "    \"strides\": 2,\n",
    "    \"padding\": \"valid\",\n",
    "    \"activation\": tf.nn.relu\n",
    "}\n",
    "\n",
    "conv1 = tf.layers.conv2d(X, name=\"conv1\", **conv1_params)\n",
    "conv2 = tf.layers.conv2d(conv1, name=\"conv2\", **conv2_params)\n",
    "\n",
    "caps1_raw = tf.reshape(conv2, [-1, caps1_n_caps, caps1_n_dims],\n",
    "                       name=\"caps1_raw\")\n",
    "\n",
    "def squash(s, axis=-1, epsilon=1e-7, name=None):\n",
    "    with tf.name_scope(name, default_name=\"squash\"):\n",
    "        squared_norm = tf.reduce_sum(tf.square(s), axis=axis,\n",
    "                                     keep_dims=True)\n",
    "        safe_norm = tf.sqrt(squared_norm + epsilon)\n",
    "        squash_factor = squared_norm / (1. + squared_norm)\n",
    "        unit_vector = s / safe_norm\n",
    "        return squash_factor * unit_vector\n",
    "\n",
    "caps1_output = squash(caps1_raw, name=\"caps1_output\")\n",
    "\n",
    "caps2_n_caps = 2\n",
    "caps2_n_dims = 16\n",
    "\n",
    "init_sigma = 0.1\n",
    "\n",
    "W_init = tf.random_normal(\n",
    "    shape=(1, caps1_n_caps, caps2_n_caps, caps2_n_dims, caps1_n_dims),\n",
    "    stddev=init_sigma, dtype=tf.float32, name=\"W_init\")\n",
    "W = tf.Variable(W_init, name=\"W\")\n",
    "\n",
    "batch_size = tf.shape(X)[0]\n",
    "W_tiled = tf.tile(W, [batch_size, 1, 1, 1, 1], name=\"W_tiled\")\n",
    "\n",
    "caps1_output_expanded = tf.expand_dims(caps1_output, -1,\n",
    "                                       name=\"caps1_output_expanded\")\n",
    "caps1_output_tile = tf.expand_dims(caps1_output_expanded, 2,\n",
    "                                   name=\"caps1_output_tile\")\n",
    "caps1_output_tiled = tf.tile(caps1_output_tile, [1, 1, caps2_n_caps, 1, 1],\n",
    "                             name=\"caps1_output_tiled\")\n",
    "\n",
    "caps2_predicted = tf.matmul(W_tiled, caps1_output_tiled,\n",
    "                            name=\"caps2_predicted\")\n",
    "\n",
    "# Dynamic Routing algorithm\n",
    "# Round 1\n",
    "raw_weights = tf.zeros([batch_size, caps1_n_caps, caps2_n_caps, 1, 1],\n",
    "                       dtype=np.float32, name=\"raw_weights\")\n",
    "routing_weights = tf.nn.softmax(raw_weights, dim=2, name=\"routing_weights\")\n",
    "\n",
    "weighted_predictions = tf.multiply(routing_weights, caps2_predicted,\n",
    "                                   name=\"weighted_predictions\")\n",
    "weighted_sum = tf.reduce_sum(weighted_predictions, axis=1, keep_dims=True,\n",
    "                             name=\"weighted_sum\")\n",
    "caps2_output_round_1 = squash(weighted_sum, axis=-2,\n",
    "                              name=\"caps2_output_round_1\")\n",
    "\n",
    "caps2_output_round_1_tiled = tf.tile(\n",
    "    caps2_output_round_1, [1, caps1_n_caps, 1, 1, 1],\n",
    "    name=\"caps2_output_round_1_tiled\")\n",
    "\n",
    "agreement1 = tf.matmul(caps2_predicted, caps2_output_round_1_tiled,\n",
    "                      transpose_a=True, name=\"agreement1\")\n",
    "# Round 2\n",
    "# Routing weight update\n",
    "raw_weights_round_2 = tf.add(raw_weights, agreement1,\n",
    "                             name=\"raw_weights_round_2\")\n",
    "routing_weights_round_2 = tf.nn.softmax(raw_weights_round_2,\n",
    "                                        dim=2,\n",
    "                                        name=\"routing_weights_round_2\")\n",
    "weighted_predictions_round_2 = tf.multiply(routing_weights_round_2,\n",
    "                                           caps2_predicted,\n",
    "                                           name=\"weighted_predictions_round_2\")\n",
    "weighted_sum_round_2 = tf.reduce_sum(weighted_predictions_round_2,\n",
    "                                     axis=1, keep_dims=True,\n",
    "                                     name=\"weighted_sum_round_2\")\n",
    "caps2_output_round_2 = squash(weighted_sum_round_2,\n",
    "                              axis=-2,\n",
    "                              name=\"caps2_output_round_2\")\n",
    "caps2_output_round_2_tiled = tf.tile(\n",
    "    caps2_output_round_2, [1, caps1_n_caps, 1, 1, 1],\n",
    "    name=\"caps2_output_round_2_tiled\")\n",
    "\n",
    "agreement2 = tf.matmul(caps2_predicted, caps2_output_round_2_tiled,\n",
    "                      transpose_a=True, name=\"agreement2\")\n",
    "\n",
    "# Round 3\n",
    "# Routing weight update\n",
    "raw_weights_round_3 = tf.add(raw_weights_round_2, agreement2,\n",
    "                             name=\"raw_weights_round_3\")\n",
    "routing_weights_round_3 = tf.nn.softmax(raw_weights_round_3,\n",
    "                                        dim=2,\n",
    "                                        name=\"routing_weights_round_3\")\n",
    "weighted_predictions_round_3 = tf.multiply(routing_weights_round_3,\n",
    "                                           caps2_predicted,\n",
    "                                           name=\"weighted_predictions_round_3\")\n",
    "weighted_sum_round_3 = tf.reduce_sum(weighted_predictions_round_3,\n",
    "                                     axis=1, keep_dims=True,\n",
    "                                     name=\"weighted_sum_round_3\")\n",
    "caps2_output_round_3 = squash(weighted_sum_round_3,\n",
    "                              axis=-2,\n",
    "                              name=\"caps2_output_round_3\")\n",
    "caps2_output_round_3_tiled = tf.tile(\n",
    "    caps2_output_round_3, [1, caps1_n_caps, 1, 1, 1],\n",
    "    name=\"caps2_output_round_3_tiled\")\n",
    "\n",
    "agreement3 = tf.matmul(caps2_predicted, caps2_output_round_3_tiled,\n",
    "                      transpose_a=True, name=\"agreement3\")\n",
    "\n",
    "# Round 4\n",
    "# Routing weight update\n",
    "raw_weights_round_4 = tf.add(raw_weights_round_3, agreement3,\n",
    "                             name=\"raw_weights_round_4\")\n",
    "routing_weights_round_4 = tf.nn.softmax(raw_weights_round_4,\n",
    "                                        dim=2,\n",
    "                                        name=\"routing_weights_round_4\")\n",
    "weighted_predictions_round_4 = tf.multiply(routing_weights_round_4,\n",
    "                                           caps2_predicted,\n",
    "                                           name=\"weighted_predictions_round_4\")\n",
    "weighted_sum_round_4 = tf.reduce_sum(weighted_predictions_round_4,\n",
    "                                     axis=1, keep_dims=True,\n",
    "                                     name=\"weighted_sum_round_4\")\n",
    "caps2_output_round_4 = squash(weighted_sum_round_4,\n",
    "                              axis=-2,\n",
    "                              name=\"caps2_output_round_4\")\n",
    "\"\"\"caps2_output_round_4_tiled = tf.tile(\n",
    "    caps2_output_round_4, [1, caps1_n_caps, 1, 1, 1],\n",
    "    name=\"caps2_output_round_4_tiled\")\n",
    "\n",
    "agreement4 = tf.matmul(caps2_predicted, caps2_output_round_4_tiled,\n",
    "                      transpose_a=True, name=\"agreement3\")\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "caps2_output = caps2_output_round_4\n",
    "\n",
    "def safe_norm(s, axis=-1, epsilon=1e-7, keep_dims=False, name=None):\n",
    "    with tf.name_scope(name, default_name=\"safe_norm\"):\n",
    "        squared_norm = tf.reduce_sum(tf.square(s), axis=axis,\n",
    "                                     keep_dims=keep_dims)\n",
    "        return tf.sqrt(squared_norm + epsilon)\n",
    "\n",
    "y_proba = safe_norm(caps2_output, axis=-2, name=\"y_proba\")\n",
    "\n",
    "\n",
    "y_proba_argmax = tf.argmax(y_proba, axis=2, name=\"y_proba\")\n",
    "\n",
    "y_pred = tf.squeeze(y_proba_argmax, axis=[1,2], name=\"y_pred\")\n",
    "\n",
    "y = tf.placeholder(shape=[None], dtype=tf.int64, name=\"y\")\n",
    "\n",
    "m_plus = 0.9\n",
    "m_minus = 0.1\n",
    "lambda_ = 0.5\n",
    "\n",
    "T = tf.one_hot(y, depth=caps2_n_caps, name=\"T\")\n",
    "\n",
    "caps2_output_norm = safe_norm(caps2_output, axis=-2, keep_dims=True,\n",
    "                              name=\"caps2_output_norm\")\n",
    "\n",
    "present_error_raw = tf.square(tf.maximum(0., m_plus - caps2_output_norm),\n",
    "                              name=\"present_error_raw\")\n",
    "present_error = tf.reshape(present_error_raw, shape=(-1, 2),\n",
    "                           name=\"present_error\")\n",
    "present_error\n",
    "\n",
    "absent_error_raw = tf.square(tf.maximum(0., caps2_output_norm - m_minus),\n",
    "                             name=\"absent_error_raw\")\n",
    "absent_error = tf.reshape(absent_error_raw, shape=(-1, 2),\n",
    "                          name=\"absent_error\")\n",
    "\n",
    "L = tf.add(T * present_error, lambda_ * (1.0 - T) * absent_error,\n",
    "           name=\"L\")\n",
    "\n",
    "margin_loss = tf.reduce_mean(tf.reduce_sum(L, axis=1), name=\"margin_loss\")\n",
    "\n",
    "mask_with_labels = tf.placeholder_with_default(False, shape=(),\n",
    "                                               name=\"mask_with_labels\")\n",
    "\n",
    "reconstruction_targets = tf.cond(mask_with_labels, # condition\n",
    "                                 lambda: y,        # if True\n",
    "                                 lambda: y_pred,   # if False\n",
    "                                 name=\"reconstruction_targets\")\n",
    "\n",
    "reconstruction_mask = tf.one_hot(reconstruction_targets,\n",
    "                                 depth=caps2_n_caps,\n",
    "                                 name=\"reconstruction_mask\")\n",
    "\n",
    "reconstruction_mask_reshaped = tf.reshape(\n",
    "    reconstruction_mask, [-1, 1, caps2_n_caps, 1, 1],\n",
    "    name=\"reconstruction_mask_reshaped\")\n",
    "\n",
    "caps2_output_masked = tf.multiply(\n",
    "    caps2_output, reconstruction_mask_reshaped,\n",
    "    name=\"caps2_output_masked\")\n",
    "\n",
    "decoder_input = tf.reshape(caps2_output_masked,\n",
    "                           [-1, caps2_n_caps * caps2_n_dims],\n",
    "                           name=\"decoder_input\")\n",
    "\n",
    "n_hidden1 = 512\n",
    "n_hidden2 = 1024\n",
    "n_output = 28 * 28\n",
    "\n",
    "with tf.name_scope(\"decoder\"):\n",
    "    hidden1 = tf.layers.dense(decoder_input, n_hidden1,\n",
    "                              activation=tf.nn.relu,\n",
    "                              name=\"hidden1\")\n",
    "    hidden2 = tf.layers.dense(hidden1, n_hidden2,\n",
    "                              activation=tf.nn.relu,\n",
    "                              name=\"hidden2\")\n",
    "    decoder_output = tf.layers.dense(hidden2, n_output,\n",
    "                                     activation=tf.nn.sigmoid,\n",
    "                                     name=\"decoder_output\")\n",
    "\n",
    "X_flat = tf.reshape(X, [-1, n_output], name=\"X_flat\")\n",
    "squared_difference = tf.square(X_flat - decoder_output,\n",
    "                               name=\"squared_difference\")\n",
    "reconstruction_loss = tf.reduce_mean(squared_difference,\n",
    "                                    name=\"reconstruction_loss\")\n",
    "\n",
    "alpha = 0.0005\n",
    "\n",
    "loss = tf.add(margin_loss, alpha * reconstruction_loss, name=\"loss\")\n",
    "\n",
    "\n",
    "correct = tf.equal(y, y_pred, name=\"correct\")\n",
    "accuracy = tf.reduce_mean(tf.cast(correct, tf.float32), name=\"accuracy\")\n",
    "\n",
    "optimizer = tf.train.AdamOptimizer()\n",
    "training_op = optimizer.minimize(loss, name=\"training_op\")\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1  Val accuracy: 99.5000%  Loss: 0.008846 (improved)\n",
      "Epoch: 2  Val accuracy: 99.8333%  Loss: 0.006639 (improved)\n",
      "Epoch: 3  Val accuracy: 99.8333%  Loss: 0.004848 (improved)\n",
      "Epoch: 4  Val accuracy: 99.8333%  Loss: 0.005155\n",
      "Epoch: 5  Val accuracy: 99.8333%  Loss: 0.004133 (improved)\n",
      "Epoch: 6  Val accuracy: 99.8333%  Loss: 0.004789\n",
      "Epoch: 7  Val accuracy: 99.8333%  Loss: 0.004406\n",
      "Epoch: 8  Val accuracy: 99.6667%  Loss: 0.004483\n",
      "Epoch: 9  Val accuracy: 99.3333%  Loss: 0.004958\n",
      "Epoch: 10  Val accuracy: 99.8333%  Loss: 0.003763 (improved)\n"
     ]
    }
   ],
   "source": [
    "n_epochs = 10\n",
    "batch_size = 50\n",
    "restore_checkpoint = False\n",
    "n_iterations_per_epoch = len(X_train) // batch_size\n",
    "n_iterations_validation = len(valX) // batch_size\n",
    "best_loss_val = np.infty\n",
    "checkpoint_path = \"./my_capsule_network0\"\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    if restore_checkpoint and tf.train.checkpoint_exists(checkpoint_path):\n",
    "        saver.restore(sess, checkpoint_path)\n",
    "    else:\n",
    "        init.run()\n",
    "\n",
    "    for epoch in range(n_epochs):\n",
    "        for iteration in range(1, n_iterations_per_epoch + 1):\n",
    "            X_batch = X_train[(iteration-1)*batch_size:(iteration*batch_size),:]\n",
    "            y_batch = Y_train[(iteration-1)*batch_size:(iteration*batch_size)]\n",
    "            # Run the training operation and measure the loss:\n",
    "            _, loss_train = sess.run(\n",
    "                [training_op, loss],\n",
    "                feed_dict={X: X_batch.reshape([-1, 28, 28, 1]),\n",
    "                           y: y_batch,\n",
    "                           mask_with_labels: True})\n",
    "            print(\"\\rIteration: {}/{} ({:.1f}%)  Loss: {:.5f}\".format(\n",
    "                      iteration, n_iterations_per_epoch,\n",
    "                      iteration * 100 / n_iterations_per_epoch,\n",
    "                      loss_train),\n",
    "                  end=\"\")\n",
    "\n",
    "        # At the end of each epoch,  \n",
    "        # measure the validation loss and accuracy:\n",
    "        loss_vals = []\n",
    "        acc_vals = []\n",
    "        for iteration in range(1, n_iterations_validation + 1):\n",
    "            X_batch = valX[(iteration-1)*batch_size:(iteration*batch_size),:]\n",
    "            y_batch = valY[(iteration-1)*batch_size:(iteration*batch_size)]\n",
    "            loss_val, acc_val = sess.run(\n",
    "                    [loss, accuracy],\n",
    "                    feed_dict={X: X_batch.reshape([-1, 28, 28, 1]),\n",
    "                               y: y_batch})\n",
    "            loss_vals.append(loss_val)\n",
    "            acc_vals.append(acc_val)\n",
    "            print(\"\\rEvaluating the model: {}/{} ({:.1f}%)\".format(\n",
    "                      iteration, n_iterations_validation,\n",
    "                      iteration * 100 / n_iterations_validation),\n",
    "                  end=\" \" * 10)\n",
    "        loss_val = np.mean(loss_vals)\n",
    "        acc_val = np.mean(acc_vals)\n",
    "        print(\"\\rEpoch: {}  Val accuracy: {:.4f}%  Loss: {:.6f}{}\".format(\n",
    "            epoch + 1, acc_val * 100, loss_val,\n",
    "            \" (improved)\" if loss_val < best_loss_val else \"\"))\n",
    "\n",
    "        # And save the model if it improved: \n",
    "        \n",
    "        if loss_val < best_loss_val:\n",
    "            save_path = saver.save(sess, checkpoint_path)\n",
    "            best_loss_val = loss_val"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./my_capsule_network0\n",
      "Final test accuracy: 99.5455%  Loss: 0.003969 \n"
     ]
    }
   ],
   "source": [
    "n_iterations_test = len(testX) // batch_size\n",
    "checkpoint_path = \"./my_capsule_network0\"\n",
    "with tf.Session() as sess:\n",
    "    saver.restore(sess, checkpoint_path)\n",
    "    pred = []\n",
    "    loss_tests = []\n",
    "    acc_tests = []\n",
    "    for iteration in range(1, n_iterations_test + 1):\n",
    "        X_batch = testX[(iteration-1)*batch_size:(iteration*batch_size),:]\n",
    "        y_batch = testY[(iteration-1)*batch_size:(iteration*batch_size)]\n",
    "        loss_test, acc_test = sess.run(\n",
    "                [loss, accuracy],\n",
    "                feed_dict={X: X_batch.reshape([-1, 28, 28, 1]),\n",
    "                           y: y_batch})\n",
    "        loss_tests.append(loss_test)\n",
    "        pred.append(y_pred)\n",
    "        acc_tests.append(acc_test)\n",
    "        print(\"\\rEvaluating the model: {}/{} ({:.1f}%)\".format(\n",
    "                  iteration, n_iterations_test,\n",
    "                  iteration * 100 / n_iterations_test),\n",
    "              end=\" \" * 10)\n",
    "    loss_test = np.mean(loss_tests)\n",
    "    #print(tf.confusion_matrix())\n",
    "    acc_test = np.mean(acc_tests)\n",
    "    print(\"\\rFinal test accuracy: {:.4f}%  Loss: {:.6f}\".format(\n",
    "        acc_test * 100, loss_test))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predictions for training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./my_capsule_network0\n",
      "\n",
      "Confusion Matrix is :\n",
      "[[5444    0]\n",
      " [   0  321]]\n",
      "\n",
      "Classification report is : \n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       1.00      1.00      1.00      5444\n",
      "        1.0       1.00      1.00      1.00       321\n",
      "\n",
      "avg / total       1.00      1.00      1.00      5765\n",
      "\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    saver.restore(sess, checkpoint_path)\n",
    "    y_pred_value = sess.run(\n",
    "            [y_pred],\n",
    "            feed_dict={X: X_train.reshape([-1, 28, 28, 1]),\n",
    "                       y: np.array([], dtype=np.int64)})\n",
    "pred = np.array(y_pred_value).T\n",
    "pred = pred.flatten()\n",
    "Y_train = Y_train.flatten()\n",
    "\n",
    "from sklearn.metrics import classification_report,confusion_matrix\n",
    "print(\"\")\n",
    "print(\"Confusion Matrix is :\")\n",
    "print(confusion_matrix(Y_train, pred, labels=None, sample_weight=None))\n",
    "print(\"\")\n",
    "print(\"Classification report is : \")\n",
    "print(\"\")\n",
    "print(classification_report(Y_train, pred, labels=None, sample_weight=None))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predictions for validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./my_capsule_network0\n",
      "\n",
      "Confusion Matrix is :\n",
      "[[479   0]\n",
      " [  1 136]]\n",
      "\n",
      "Classification report is : \n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       1.00      1.00      1.00       479\n",
      "        1.0       1.00      0.99      1.00       137\n",
      "\n",
      "avg / total       1.00      1.00      1.00       616\n",
      "\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    saver.restore(sess, checkpoint_path)\n",
    "    y_pred_value = sess.run(\n",
    "            [y_pred],\n",
    "            feed_dict={X: valX.reshape([-1, 28, 28, 1]),\n",
    "                       y: np.array([], dtype=np.int64)})\n",
    "pred = np.array(y_pred_value).T\n",
    "pred = pred.flatten()\n",
    "valY = valY.flatten()\n",
    "\n",
    "from sklearn.metrics import classification_report,confusion_matrix\n",
    "print(\"\")\n",
    "print(\"Confusion Matrix is :\")\n",
    "print(confusion_matrix(valY, pred, labels=None, sample_weight=None))\n",
    "print(\"\")\n",
    "print(\"Classification report is : \")\n",
    "print(\"\")\n",
    "print(classification_report(valY, pred, labels=None, sample_weight=None))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test set prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./my_capsule_network0\n",
      "\n",
      "Accuracy is :  0.9946284691136974\n",
      "\n",
      "AUROC is :  0.9781021897810219\n",
      "\n",
      "Confusion Matrix is :\n",
      "[[980   0]\n",
      " [  6 131]]\n",
      "\n",
      "Classification report is : \n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.99      1.00      1.00       980\n",
      "        1.0       1.00      0.96      0.98       137\n",
      "\n",
      "avg / total       0.99      0.99      0.99      1117\n",
      "\n"
     ]
    }
   ],
   "source": [
    "with tf.Session(config=tf.ConfigProto(\n",
    "      allow_soft_placement=True, log_device_placement=True)) as sess:\n",
    "    saver.restore(sess, checkpoint_path)\n",
    "    y_pred_value = sess.run(\n",
    "            [y_pred],\n",
    "            feed_dict={X: testX.reshape([-1, 28, 28, 1]),\n",
    "                       y: np.array([], dtype=np.int64)})\n",
    "pred = np.array(y_pred_value).T\n",
    "pred = pred.flatten()\n",
    "testY = testY.flatten()\n",
    "\n",
    "from sklearn.metrics import classification_report,confusion_matrix,roc_auc_score,accuracy_score\n",
    "print(\"\")\n",
    "print(\"Accuracy is : \", accuracy_score(testY,pred))\n",
    "print(\"\")\n",
    "print(\"AUROC is : \", roc_auc_score(testY,pred))\n",
    "print(\"\")\n",
    "print(\"Confusion Matrix is :\")\n",
    "print(confusion_matrix(testY, pred, labels=None, sample_weight=None))\n",
    "print(\"\")\n",
    "print(\"Classification report is : \")\n",
    "print(\"\")\n",
    "print(classification_report(testY, pred, labels=None, sample_weight=None))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AUROC\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./my_capsule_network0\n",
      "AUROC is :  0.9999627588261581\n",
      "\n"
     ]
    }
   ],
   "source": [
    "with tf.Session(config=tf.ConfigProto(\n",
    "      allow_soft_placement=True, log_device_placement=True)) as sess:\n",
    "    saver.restore(sess, checkpoint_path)\n",
    "    prob = sess.run(\n",
    "            [y_proba],\n",
    "            feed_dict={X: testX.reshape([-1, 28, 28, 1]),\n",
    "                       y: np.array([], dtype=np.int64)})\n",
    "prob = np.array(prob)\n",
    "probs = np.array(prob[0,:,0,1,0])\n",
    "probs = probs.flatten()\n",
    "testY = testY.flatten()\n",
    "\n",
    "from sklearn.metrics import classification_report,confusion_matrix,roc_auc_score,accuracy_score\n",
    "print(\"AUROC is : \", roc_auc_score(testY,probs))\n",
    "print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
