{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/inderjeet78/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "from __future__ import division, print_function, unicode_literals\n",
    "\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "tf.reset_default_graph()\n",
    "\n",
    "np.random.seed(42)\n",
    "tf.set_random_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-2-4141630e56b4>:3: read_data_sets (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n",
      "WARNING:tensorflow:From /home/inderjeet78/anaconda3/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:260: maybe_download (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please write your own downloading logic.\n",
      "WARNING:tensorflow:From /home/inderjeet78/anaconda3/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:262: extract_images (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting /tmp/data/train-images-idx3-ubyte.gz\n",
      "WARNING:tensorflow:From /home/inderjeet78/anaconda3/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:267: extract_labels (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting /tmp/data/train-labels-idx1-ubyte.gz\n",
      "Extracting /tmp/data/t10k-images-idx3-ubyte.gz\n",
      "Extracting /tmp/data/t10k-labels-idx1-ubyte.gz\n",
      "WARNING:tensorflow:From /home/inderjeet78/anaconda3/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:290: DataSet.__init__ (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "\n",
    "mnist = input_data.read_data_sets(\"/tmp/data/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training data prepration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5715, 785)\n",
      "(49285, 785)\n",
      "[[0. 0. 0. ... 0. 0. 3.]\n",
      " [0. 0. 0. ... 0. 0. 4.]\n",
      " [0. 0. 0. ... 0. 0. 6.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 5.]\n",
      " [0. 0. 0. ... 0. 0. 6.]\n",
      " [0. 0. 0. ... 0. 0. 8.]]\n",
      "(49285, 785)\n",
      "(321, 785)\n"
     ]
    }
   ],
   "source": [
    "c1_x = mnist.train.images[mnist.train.labels==7]\n",
    "c1_y = mnist.train.labels[mnist.train.labels==7]\n",
    "c1_y = c1_y[:,None]\n",
    "other_x = mnist.train.images[mnist.train.labels!=7]\n",
    "other_y = mnist.train.labels[mnist.train.labels!=7]\n",
    "other_y=other_y[:,None]\n",
    "\n",
    "np.random.seed(42)\n",
    "c1 = np.concatenate((c1_x,c1_y),axis=1)\n",
    "others = np.concatenate((other_x,other_y), axis=1)\n",
    "print(c1.shape)\n",
    "print(others.shape)\n",
    "print(others)\n",
    "np.random.shuffle(others)\n",
    "others = np.array(others)\n",
    "print(others.shape)\n",
    "others321 = others[0:321,:]\n",
    "print(others321.shape)\n",
    "train = np.concatenate((c1,others321),axis=0)\n",
    "np.random.shuffle(train)\n",
    "X_train = train[:,0:-1]\n",
    "Y_train = train[:,-1]\n",
    "Y_train[Y_train==0]=1\n",
    "Y_train[Y_train==7]=0\n",
    "Y_train[Y_train!=0]=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "321.0"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(Y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Validation data prepration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "valX_ones = mnist.validation.images[mnist.validation.labels==7]\n",
    "valY_ones = mnist.validation.labels[mnist.validation.labels==7]\n",
    "valX_others = mnist.validation.images[mnist.validation.labels!=7]\n",
    "valY_others = mnist.validation.labels[mnist.validation.labels!=7]\n",
    "valY_ones = valY_ones[:,None]\n",
    "valY_others = valY_others[:,None]\n",
    "val_ones = np.concatenate((valX_ones,valY_ones),axis=1)\n",
    "val_others = np.concatenate((valX_others,valY_others),axis=1)\n",
    "np.random.shuffle(val_others)\n",
    "val_others137 = val_others[0:137,:]\n",
    "val = np.concatenate((val_ones,val_others137),axis=0)\n",
    "np.random.shuffle(val)\n",
    "valX = val[:,0:-1]\n",
    "valY = val[:,-1]\n",
    "valY[valY==0]=1\n",
    "valY[valY==7]=0\n",
    "valY[valY!=0]=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "137.0"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(valY)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test data prepration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "testX_ones = mnist.test.images[mnist.test.labels==7]\n",
    "testY_ones = mnist.test.labels[mnist.test.labels==7]\n",
    "testX_others = mnist.test.images[mnist.test.labels!=7]\n",
    "testY_others = mnist.test.labels[mnist.test.labels!=7]\n",
    "testY_ones = testY_ones[:,None]\n",
    "testY_others = testY_others[:,None]\n",
    "test_ones = np.concatenate((testX_ones,testY_ones),axis=1)\n",
    "test_others = np.concatenate((testX_others,testY_others),axis=1)\n",
    "np.random.shuffle(test_others)\n",
    "test_others137 = test_others[0:137,:]\n",
    "test = np.concatenate((test_ones,test_others137),axis=0)\n",
    "np.random.shuffle(test)\n",
    "testX = test[:,0:-1]\n",
    "testY = test[:,-1]\n",
    "testY[testY==0]=1\n",
    "testY[testY==7]=0\n",
    "testY[testY!=0]=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "137.0"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(testY)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data checking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 3600x216 with 11 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "n_samples = 25\n",
    "\n",
    "plt.figure(figsize=(n_samples * 2, 3))\n",
    "for index in range(14,n_samples):\n",
    "    plt.subplot(1, n_samples, index + 1)\n",
    "    sample_image = X_train[index].reshape(28, 28)\n",
    "    plt.imshow(sample_image, cmap=\"binary\")\n",
    "    plt.axis(\"off\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0.], dtype=float32)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_train[:n_samples]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparing Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-11-d9e33cb66a21>:32: calling reduce_sum (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "keep_dims is deprecated, use keepdims instead\n",
      "WARNING:tensorflow:From <ipython-input-11-d9e33cb66a21>:67: calling softmax (from tensorflow.python.ops.nn_ops) with dim is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "dim is deprecated, use axis instead\n"
     ]
    }
   ],
   "source": [
    "X = tf.placeholder(shape=[None, 28, 28, 1], dtype=tf.float32, name=\"X\")\n",
    "\n",
    "caps1_n_maps = 32\n",
    "caps1_n_caps = caps1_n_maps * 6 * 6  # 1152 primary capsules\n",
    "caps1_n_dims = 8\n",
    "\n",
    "conv1_params = {\n",
    "    \"filters\": 256,\n",
    "    \"kernel_size\": 9,\n",
    "    \"strides\": 1,\n",
    "    \"padding\": \"valid\",\n",
    "    \"activation\": tf.nn.relu,\n",
    "}\n",
    "\n",
    "conv2_params = {\n",
    "    \"filters\": caps1_n_maps * caps1_n_dims, # 256 convolutional filters\n",
    "    \"kernel_size\": 9,\n",
    "    \"strides\": 2,\n",
    "    \"padding\": \"valid\",\n",
    "    \"activation\": tf.nn.relu\n",
    "}\n",
    "\n",
    "conv1 = tf.layers.conv2d(X, name=\"conv1\", **conv1_params)\n",
    "conv2 = tf.layers.conv2d(conv1, name=\"conv2\", **conv2_params)\n",
    "\n",
    "caps1_raw = tf.reshape(conv2, [-1, caps1_n_caps, caps1_n_dims],\n",
    "                       name=\"caps1_raw\")\n",
    "\n",
    "def squash(s, axis=-1, epsilon=1e-7, name=None):\n",
    "    with tf.name_scope(name, default_name=\"squash\"):\n",
    "        squared_norm = tf.reduce_sum(tf.square(s), axis=axis,\n",
    "                                     keep_dims=True)\n",
    "        safe_norm = tf.sqrt(squared_norm + epsilon)\n",
    "        squash_factor = squared_norm / (1. + squared_norm)\n",
    "        unit_vector = s / safe_norm\n",
    "        return squash_factor * unit_vector\n",
    "\n",
    "caps1_output = squash(caps1_raw, name=\"caps1_output\")\n",
    "\n",
    "caps2_n_caps = 2\n",
    "caps2_n_dims = 16\n",
    "\n",
    "init_sigma = 0.1\n",
    "\n",
    "W_init = tf.random_normal(\n",
    "    shape=(1, caps1_n_caps, caps2_n_caps, caps2_n_dims, caps1_n_dims),\n",
    "    stddev=init_sigma, dtype=tf.float32, name=\"W_init\")\n",
    "W = tf.Variable(W_init, name=\"W\")\n",
    "\n",
    "batch_size = tf.shape(X)[0]\n",
    "W_tiled = tf.tile(W, [batch_size, 1, 1, 1, 1], name=\"W_tiled\")\n",
    "\n",
    "caps1_output_expanded = tf.expand_dims(caps1_output, -1,\n",
    "                                       name=\"caps1_output_expanded\")\n",
    "caps1_output_tile = tf.expand_dims(caps1_output_expanded, 2,\n",
    "                                   name=\"caps1_output_tile\")\n",
    "caps1_output_tiled = tf.tile(caps1_output_tile, [1, 1, caps2_n_caps, 1, 1],\n",
    "                             name=\"caps1_output_tiled\")\n",
    "\n",
    "caps2_predicted = tf.matmul(W_tiled, caps1_output_tiled,\n",
    "                            name=\"caps2_predicted\")\n",
    "\n",
    "# Dynamic Routing algorithm\n",
    "# Round 1\n",
    "raw_weights = tf.zeros([batch_size, caps1_n_caps, caps2_n_caps, 1, 1],\n",
    "                       dtype=np.float32, name=\"raw_weights\")\n",
    "routing_weights = tf.nn.softmax(raw_weights, dim=2, name=\"routing_weights\")\n",
    "\n",
    "weighted_predictions = tf.multiply(routing_weights, caps2_predicted,\n",
    "                                   name=\"weighted_predictions\")\n",
    "weighted_sum = tf.reduce_sum(weighted_predictions, axis=1, keep_dims=True,\n",
    "                             name=\"weighted_sum\")\n",
    "caps2_output_round_1 = squash(weighted_sum, axis=-2,\n",
    "                              name=\"caps2_output_round_1\")\n",
    "\n",
    "caps2_output_round_1_tiled = tf.tile(\n",
    "    caps2_output_round_1, [1, caps1_n_caps, 1, 1, 1],\n",
    "    name=\"caps2_output_round_1_tiled\")\n",
    "\n",
    "agreement1 = tf.matmul(caps2_predicted, caps2_output_round_1_tiled,\n",
    "                      transpose_a=True, name=\"agreement1\")\n",
    "# Round 2\n",
    "# Routing weight update\n",
    "raw_weights_round_2 = tf.add(raw_weights, agreement1,\n",
    "                             name=\"raw_weights_round_2\")\n",
    "routing_weights_round_2 = tf.nn.softmax(raw_weights_round_2,\n",
    "                                        dim=2,\n",
    "                                        name=\"routing_weights_round_2\")\n",
    "weighted_predictions_round_2 = tf.multiply(routing_weights_round_2,\n",
    "                                           caps2_predicted,\n",
    "                                           name=\"weighted_predictions_round_2\")\n",
    "weighted_sum_round_2 = tf.reduce_sum(weighted_predictions_round_2,\n",
    "                                     axis=1, keep_dims=True,\n",
    "                                     name=\"weighted_sum_round_2\")\n",
    "caps2_output_round_2 = squash(weighted_sum_round_2,\n",
    "                              axis=-2,\n",
    "                              name=\"caps2_output_round_2\")\n",
    "caps2_output_round_2_tiled = tf.tile(\n",
    "    caps2_output_round_2, [1, caps1_n_caps, 1, 1, 1],\n",
    "    name=\"caps2_output_round_2_tiled\")\n",
    "\n",
    "agreement2 = tf.matmul(caps2_predicted, caps2_output_round_2_tiled,\n",
    "                      transpose_a=True, name=\"agreement2\")\n",
    "\n",
    "# Round 3\n",
    "# Routing weight update\n",
    "raw_weights_round_3 = tf.add(raw_weights_round_2, agreement2,\n",
    "                             name=\"raw_weights_round_3\")\n",
    "routing_weights_round_3 = tf.nn.softmax(raw_weights_round_3,\n",
    "                                        dim=2,\n",
    "                                        name=\"routing_weights_round_3\")\n",
    "weighted_predictions_round_3 = tf.multiply(routing_weights_round_3,\n",
    "                                           caps2_predicted,\n",
    "                                           name=\"weighted_predictions_round_3\")\n",
    "weighted_sum_round_3 = tf.reduce_sum(weighted_predictions_round_3,\n",
    "                                     axis=1, keep_dims=True,\n",
    "                                     name=\"weighted_sum_round_3\")\n",
    "caps2_output_round_3 = squash(weighted_sum_round_3,\n",
    "                              axis=-2,\n",
    "                              name=\"caps2_output_round_3\")\n",
    "caps2_output_round_3_tiled = tf.tile(\n",
    "    caps2_output_round_3, [1, caps1_n_caps, 1, 1, 1],\n",
    "    name=\"caps2_output_round_3_tiled\")\n",
    "\n",
    "agreement3 = tf.matmul(caps2_predicted, caps2_output_round_3_tiled,\n",
    "                      transpose_a=True, name=\"agreement3\")\n",
    "\n",
    "# Round 4\n",
    "# Routing weight update\n",
    "raw_weights_round_4 = tf.add(raw_weights_round_3, agreement3,\n",
    "                             name=\"raw_weights_round_4\")\n",
    "routing_weights_round_4 = tf.nn.softmax(raw_weights_round_4,\n",
    "                                        dim=2,\n",
    "                                        name=\"routing_weights_round_4\")\n",
    "weighted_predictions_round_4 = tf.multiply(routing_weights_round_4,\n",
    "                                           caps2_predicted,\n",
    "                                           name=\"weighted_predictions_round_4\")\n",
    "weighted_sum_round_4 = tf.reduce_sum(weighted_predictions_round_4,\n",
    "                                     axis=1, keep_dims=True,\n",
    "                                     name=\"weighted_sum_round_4\")\n",
    "caps2_output_round_4 = squash(weighted_sum_round_4,\n",
    "                              axis=-2,\n",
    "                              name=\"caps2_output_round_4\")\n",
    "\"\"\"caps2_output_round_4_tiled = tf.tile(\n",
    "    caps2_output_round_4, [1, caps1_n_caps, 1, 1, 1],\n",
    "    name=\"caps2_output_round_4_tiled\")\n",
    "\n",
    "agreement4 = tf.matmul(caps2_predicted, caps2_output_round_4_tiled,\n",
    "                      transpose_a=True, name=\"agreement3\")\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "caps2_output = caps2_output_round_4\n",
    "\n",
    "def safe_norm(s, axis=-1, epsilon=1e-7, keep_dims=False, name=None):\n",
    "    with tf.name_scope(name, default_name=\"safe_norm\"):\n",
    "        squared_norm = tf.reduce_sum(tf.square(s), axis=axis,\n",
    "                                     keep_dims=keep_dims)\n",
    "        return tf.sqrt(squared_norm + epsilon)\n",
    "\n",
    "y_proba = safe_norm(caps2_output, axis=-2, name=\"y_proba\")\n",
    "\n",
    "\n",
    "y_proba_argmax = tf.argmax(y_proba, axis=2, name=\"y_proba\")\n",
    "\n",
    "y_pred = tf.squeeze(y_proba_argmax, axis=[1,2], name=\"y_pred\")\n",
    "\n",
    "y = tf.placeholder(shape=[None], dtype=tf.int64, name=\"y\")\n",
    "\n",
    "m_plus = 0.9\n",
    "m_minus = 0.1\n",
    "lambda_ = 0.5\n",
    "\n",
    "T = tf.one_hot(y, depth=caps2_n_caps, name=\"T\")\n",
    "\n",
    "caps2_output_norm = safe_norm(caps2_output, axis=-2, keep_dims=True,\n",
    "                              name=\"caps2_output_norm\")\n",
    "\n",
    "present_error_raw = tf.square(tf.maximum(0., m_plus - caps2_output_norm),\n",
    "                              name=\"present_error_raw\")\n",
    "present_error = tf.reshape(present_error_raw, shape=(-1, 2),\n",
    "                           name=\"present_error\")\n",
    "present_error\n",
    "\n",
    "absent_error_raw = tf.square(tf.maximum(0., caps2_output_norm - m_minus),\n",
    "                             name=\"absent_error_raw\")\n",
    "absent_error = tf.reshape(absent_error_raw, shape=(-1, 2),\n",
    "                          name=\"absent_error\")\n",
    "\n",
    "L = tf.add(T * present_error, lambda_ * (1.0 - T) * absent_error,\n",
    "           name=\"L\")\n",
    "\n",
    "margin_loss = tf.reduce_mean(tf.reduce_sum(L, axis=1), name=\"margin_loss\")\n",
    "\n",
    "mask_with_labels = tf.placeholder_with_default(False, shape=(),\n",
    "                                               name=\"mask_with_labels\")\n",
    "\n",
    "reconstruction_targets = tf.cond(mask_with_labels, # condition\n",
    "                                 lambda: y,        # if True\n",
    "                                 lambda: y_pred,   # if False\n",
    "                                 name=\"reconstruction_targets\")\n",
    "\n",
    "reconstruction_mask = tf.one_hot(reconstruction_targets,\n",
    "                                 depth=caps2_n_caps,\n",
    "                                 name=\"reconstruction_mask\")\n",
    "\n",
    "reconstruction_mask_reshaped = tf.reshape(\n",
    "    reconstruction_mask, [-1, 1, caps2_n_caps, 1, 1],\n",
    "    name=\"reconstruction_mask_reshaped\")\n",
    "\n",
    "caps2_output_masked = tf.multiply(\n",
    "    caps2_output, reconstruction_mask_reshaped,\n",
    "    name=\"caps2_output_masked\")\n",
    "\n",
    "decoder_input = tf.reshape(caps2_output_masked,\n",
    "                           [-1, caps2_n_caps * caps2_n_dims],\n",
    "                           name=\"decoder_input\")\n",
    "\n",
    "n_hidden1 = 512\n",
    "n_hidden2 = 1024\n",
    "n_output = 28 * 28\n",
    "\n",
    "with tf.name_scope(\"decoder\"):\n",
    "    hidden1 = tf.layers.dense(decoder_input, n_hidden1,\n",
    "                              activation=tf.nn.relu,\n",
    "                              name=\"hidden1\")\n",
    "    hidden2 = tf.layers.dense(hidden1, n_hidden2,\n",
    "                              activation=tf.nn.relu,\n",
    "                              name=\"hidden2\")\n",
    "    decoder_output = tf.layers.dense(hidden2, n_output,\n",
    "                                     activation=tf.nn.sigmoid,\n",
    "                                     name=\"decoder_output\")\n",
    "\n",
    "X_flat = tf.reshape(X, [-1, n_output], name=\"X_flat\")\n",
    "squared_difference = tf.square(X_flat - decoder_output,\n",
    "                               name=\"squared_difference\")\n",
    "reconstruction_loss = tf.reduce_mean(squared_difference,\n",
    "                                    name=\"reconstruction_loss\")\n",
    "\n",
    "alpha = 0.0005\n",
    "\n",
    "loss = tf.add(margin_loss, alpha * reconstruction_loss, name=\"loss\")\n",
    "\n",
    "\n",
    "correct = tf.equal(y, y_pred, name=\"correct\")\n",
    "accuracy = tf.reduce_mean(tf.cast(correct, tf.float32), name=\"accuracy\")\n",
    "\n",
    "optimizer = tf.train.AdamOptimizer()\n",
    "training_op = optimizer.minimize(loss, name=\"training_op\")\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1  Val accuracy: 96.9231%  Loss: 0.022268 (improved)\n",
      "Epoch: 2  Val accuracy: 98.4615%  Loss: 0.013112 (improved)\n",
      "Epoch: 3  Val accuracy: 98.7692%  Loss: 0.010854 (improved)\n",
      "Epoch: 4  Val accuracy: 98.6154%  Loss: 0.009600 (improved)\n",
      "Epoch: 5  Val accuracy: 98.7692%  Loss: 0.010518\n",
      "Epoch: 6  Val accuracy: 98.7692%  Loss: 0.010301\n",
      "Epoch: 7  Val accuracy: 97.6923%  Loss: 0.015568\n",
      "Epoch: 8  Val accuracy: 98.6154%  Loss: 0.012135\n",
      "Epoch: 9  Val accuracy: 98.9231%  Loss: 0.009136 (improved)\n",
      "Epoch: 10  Val accuracy: 98.7692%  Loss: 0.010942\n"
     ]
    }
   ],
   "source": [
    "n_epochs = 10\n",
    "batch_size = 50\n",
    "restore_checkpoint = False\n",
    "n_iterations_per_epoch = len(X_train) // batch_size\n",
    "n_iterations_validation = len(valX) // batch_size\n",
    "best_loss_val = np.infty\n",
    "checkpoint_path = \"./my_capsule_network7\"\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    if restore_checkpoint and tf.train.checkpoint_exists(checkpoint_path):\n",
    "        saver.restore(sess, checkpoint_path)\n",
    "    else:\n",
    "        init.run()\n",
    "\n",
    "    for epoch in range(n_epochs):\n",
    "        for iteration in range(1, n_iterations_per_epoch + 1):\n",
    "            X_batch = X_train[(iteration-1)*batch_size:(iteration*batch_size),:]\n",
    "            y_batch = Y_train[(iteration-1)*batch_size:(iteration*batch_size)]\n",
    "            # Run the training operation and measure the loss:\n",
    "            _, loss_train = sess.run(\n",
    "                [training_op, loss],\n",
    "                feed_dict={X: X_batch.reshape([-1, 28, 28, 1]),\n",
    "                           y: y_batch,\n",
    "                           mask_with_labels: True})\n",
    "            print(\"\\rIteration: {}/{} ({:.1f}%)  Loss: {:.5f}\".format(\n",
    "                      iteration, n_iterations_per_epoch,\n",
    "                      iteration * 100 / n_iterations_per_epoch,\n",
    "                      loss_train),\n",
    "                  end=\"\")\n",
    "\n",
    "        # At the end of each epoch,  \n",
    "        # measure the validation loss and accuracy:\n",
    "        loss_vals = []\n",
    "        acc_vals = []\n",
    "        for iteration in range(1, n_iterations_validation + 1):\n",
    "            X_batch = valX[(iteration-1)*batch_size:(iteration*batch_size),:]\n",
    "            y_batch = valY[(iteration-1)*batch_size:(iteration*batch_size)]\n",
    "            loss_val, acc_val = sess.run(\n",
    "                    [loss, accuracy],\n",
    "                    feed_dict={X: X_batch.reshape([-1, 28, 28, 1]),\n",
    "                               y: y_batch})\n",
    "            loss_vals.append(loss_val)\n",
    "            acc_vals.append(acc_val)\n",
    "            print(\"\\rEvaluating the model: {}/{} ({:.1f}%)\".format(\n",
    "                      iteration, n_iterations_validation,\n",
    "                      iteration * 100 / n_iterations_validation),\n",
    "                  end=\" \" * 10)\n",
    "        loss_val = np.mean(loss_vals)\n",
    "        acc_val = np.mean(acc_vals)\n",
    "        print(\"\\rEpoch: {}  Val accuracy: {:.4f}%  Loss: {:.6f}{}\".format(\n",
    "            epoch + 1, acc_val * 100, loss_val,\n",
    "            \" (improved)\" if loss_val < best_loss_val else \"\"))\n",
    "\n",
    "        # And save the model if it improved: \n",
    "        \n",
    "        if loss_val < best_loss_val:\n",
    "            save_path = saver.save(sess, checkpoint_path)\n",
    "            best_loss_val = loss_val"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./my_capsule_network7\n",
      "Final test accuracy: 98.8696%  Loss: 0.006388 \n"
     ]
    }
   ],
   "source": [
    "n_iterations_test = len(testX) // batch_size\n",
    "checkpoint_path = \"./my_capsule_network7\"\n",
    "with tf.Session() as sess:\n",
    "    saver.restore(sess, checkpoint_path)\n",
    "    pred = []\n",
    "    loss_tests = []\n",
    "    acc_tests = []\n",
    "    for iteration in range(1, n_iterations_test + 1):\n",
    "        X_batch = testX[(iteration-1)*batch_size:(iteration*batch_size),:]\n",
    "        y_batch = testY[(iteration-1)*batch_size:(iteration*batch_size)]\n",
    "        loss_test, acc_test = sess.run(\n",
    "                [loss, accuracy],\n",
    "                feed_dict={X: X_batch.reshape([-1, 28, 28, 1]),\n",
    "                           y: y_batch})\n",
    "        loss_tests.append(loss_test)\n",
    "        pred.append(y_pred)\n",
    "        acc_tests.append(acc_test)\n",
    "        print(\"\\rEvaluating the model: {}/{} ({:.1f}%)\".format(\n",
    "                  iteration, n_iterations_test,\n",
    "                  iteration * 100 / n_iterations_test),\n",
    "              end=\" \" * 10)\n",
    "    loss_test = np.mean(loss_tests)\n",
    "    #print(tf.confusion_matrix())\n",
    "    acc_test = np.mean(acc_tests)\n",
    "    print(\"\\rFinal test accuracy: {:.4f}%  Loss: {:.6f}\".format(\n",
    "        acc_test * 100, loss_test))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predictions for training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./my_capsule_network7\n",
      "\n",
      "Confusion Matrix is :\n",
      "[[5715    0]\n",
      " [   0  321]]\n",
      "\n",
      "Classification report is : \n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       1.00      1.00      1.00      5715\n",
      "        1.0       1.00      1.00      1.00       321\n",
      "\n",
      "avg / total       1.00      1.00      1.00      6036\n",
      "\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    saver.restore(sess, checkpoint_path)\n",
    "    y_pred_value = sess.run(\n",
    "            [y_pred],\n",
    "            feed_dict={X: X_train.reshape([-1, 28, 28, 1]),\n",
    "                       y: np.array([], dtype=np.int64)})\n",
    "pred = np.array(y_pred_value).T\n",
    "pred = pred.flatten()\n",
    "Y_train = Y_train.flatten()\n",
    "\n",
    "from sklearn.metrics import classification_report,confusion_matrix\n",
    "print(\"\")\n",
    "print(\"Confusion Matrix is :\")\n",
    "print(confusion_matrix(Y_train, pred, labels=None, sample_weight=None))\n",
    "print(\"\")\n",
    "print(\"Classification report is : \")\n",
    "print(\"\")\n",
    "print(classification_report(Y_train, pred, labels=None, sample_weight=None))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predictions for validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./my_capsule_network7\n",
      "\n",
      "Confusion Matrix is :\n",
      "[[549   1]\n",
      " [  6 131]]\n",
      "\n",
      "Classification report is : \n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.99      1.00      0.99       550\n",
      "        1.0       0.99      0.96      0.97       137\n",
      "\n",
      "avg / total       0.99      0.99      0.99       687\n",
      "\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    saver.restore(sess, checkpoint_path)\n",
    "    y_pred_value = sess.run(\n",
    "            [y_pred],\n",
    "            feed_dict={X: valX.reshape([-1, 28, 28, 1]),\n",
    "                       y: np.array([], dtype=np.int64)})\n",
    "pred = np.array(y_pred_value).T\n",
    "pred = pred.flatten()\n",
    "valY = valY.flatten()\n",
    "\n",
    "from sklearn.metrics import classification_report,confusion_matrix\n",
    "print(\"\")\n",
    "print(\"Confusion Matrix is :\")\n",
    "print(confusion_matrix(valY, pred, labels=None, sample_weight=None))\n",
    "print(\"\")\n",
    "print(\"Classification report is : \")\n",
    "print(\"\")\n",
    "print(classification_report(valY, pred, labels=None, sample_weight=None))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test set prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./my_capsule_network7\n",
      "\n",
      "Accuracy is :  0.9888412017167382\n",
      "\n",
      "AUROC is :  0.9588812519526257\n",
      "\n",
      "Confusion Matrix is :\n",
      "[[1026    2]\n",
      " [  11  126]]\n",
      "\n",
      "Classification report is : \n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.99      1.00      0.99      1028\n",
      "        1.0       0.98      0.92      0.95       137\n",
      "\n",
      "avg / total       0.99      0.99      0.99      1165\n",
      "\n"
     ]
    }
   ],
   "source": [
    "with tf.Session(config=tf.ConfigProto(\n",
    "      allow_soft_placement=True, log_device_placement=True)) as sess:\n",
    "    saver.restore(sess, checkpoint_path)\n",
    "    y_pred_value = sess.run(\n",
    "            [y_pred],\n",
    "            feed_dict={X: testX.reshape([-1, 28, 28, 1]),\n",
    "                       y: np.array([], dtype=np.int64)})\n",
    "pred = np.array(y_pred_value).T\n",
    "pred = pred.flatten()\n",
    "testY = testY.flatten()\n",
    "\n",
    "from sklearn.metrics import classification_report,confusion_matrix,roc_auc_score,accuracy_score\n",
    "print(\"\")\n",
    "print(\"Accuracy is : \", accuracy_score(testY,pred))\n",
    "print(\"\")\n",
    "print(\"AUROC is : \", roc_auc_score(testY,pred))\n",
    "print(\"\")\n",
    "print(\"Confusion Matrix is :\")\n",
    "print(confusion_matrix(testY, pred, labels=None, sample_weight=None))\n",
    "print(\"\")\n",
    "print(\"Classification report is : \")\n",
    "print(\"\")\n",
    "print(classification_report(testY, pred, labels=None, sample_weight=None))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AUROC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./my_capsule_network7\n",
      "AUROC is :  0.9997869862819165\n",
      "\n"
     ]
    }
   ],
   "source": [
    "checkpoint_path = \"./my_capsule_network7\"\n",
    "with tf.Session(config=tf.ConfigProto(\n",
    "      allow_soft_placement=True, log_device_placement=True)) as sess:\n",
    "    saver.restore(sess, checkpoint_path)\n",
    "    prob = sess.run(\n",
    "            [y_proba],\n",
    "            feed_dict={X: testX.reshape([-1, 28, 28, 1]),\n",
    "                       y: np.array([], dtype=np.int64)})\n",
    "prob = np.array(prob)\n",
    "probs = np.array(prob[0,:,0,1,0])\n",
    "probs = probs.flatten()\n",
    "testY = testY.flatten()\n",
    "\n",
    "from sklearn.metrics import classification_report,confusion_matrix,roc_auc_score,accuracy_score\n",
    "print(\"AUROC is : \", roc_auc_score(testY,probs))\n",
    "print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
