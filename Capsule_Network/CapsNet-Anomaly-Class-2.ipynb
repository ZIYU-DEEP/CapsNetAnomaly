{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/inderjeet78/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-1-88ab3af243eb>:17: read_data_sets (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n",
      "WARNING:tensorflow:From /home/inderjeet78/anaconda3/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:260: maybe_download (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please write your own downloading logic.\n",
      "WARNING:tensorflow:From /home/inderjeet78/anaconda3/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:262: extract_images (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting /tmp/data/train-images-idx3-ubyte.gz\n",
      "WARNING:tensorflow:From /home/inderjeet78/anaconda3/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:267: extract_labels (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting /tmp/data/train-labels-idx1-ubyte.gz\n",
      "Extracting /tmp/data/t10k-images-idx3-ubyte.gz\n",
      "Extracting /tmp/data/t10k-labels-idx1-ubyte.gz\n",
      "WARNING:tensorflow:From /home/inderjeet78/anaconda3/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:290: DataSet.__init__ (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n",
      "(5470, 785)\n",
      "(49530, 785)\n",
      "[[0. 0. 0. ... 0. 0. 7.]\n",
      " [0. 0. 0. ... 0. 0. 3.]\n",
      " [0. 0. 0. ... 0. 0. 4.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 5.]\n",
      " [0. 0. 0. ... 0. 0. 6.]\n",
      " [0. 0. 0. ... 0. 0. 8.]]\n",
      "(49530, 785)\n",
      "(321, 785)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "137.0"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from __future__ import division, print_function, unicode_literals\n",
    "\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "tf.reset_default_graph()\n",
    "\n",
    "np.random.seed(42)\n",
    "tf.set_random_seed(42)\n",
    "\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "\n",
    "mnist = input_data.read_data_sets(\"/tmp/data/\")\n",
    "\n",
    "# Training data prepration\n",
    "\n",
    "c1_x = mnist.train.images[mnist.train.labels==2]\n",
    "c1_y = mnist.train.labels[mnist.train.labels==2]\n",
    "c1_y = c1_y[:,None]\n",
    "other_x = mnist.train.images[mnist.train.labels!=2]\n",
    "other_y = mnist.train.labels[mnist.train.labels!=2]\n",
    "other_y=other_y[:,None]\n",
    "\n",
    "np.random.seed(42)\n",
    "c1 = np.concatenate((c1_x,c1_y),axis=1)\n",
    "others = np.concatenate((other_x,other_y), axis=1)\n",
    "print(c1.shape)\n",
    "print(others.shape)\n",
    "print(others)\n",
    "np.random.shuffle(others)\n",
    "others = np.array(others)\n",
    "print(others.shape)\n",
    "others321 = others[0:321,:]\n",
    "print(others321.shape)\n",
    "train = np.concatenate((c1,others321),axis=0)\n",
    "np.random.shuffle(train)\n",
    "X_train = train[:,0:-1]\n",
    "Y_train = train[:,-1]\n",
    "Y_train[Y_train==0]=1\n",
    "Y_train[Y_train==2]=0\n",
    "Y_train[Y_train!=0]=1\n",
    "\n",
    "sum(Y_train)\n",
    "\n",
    "# Validation data prepration\n",
    "\n",
    "np.random.seed(42)\n",
    "valX_ones = mnist.validation.images[mnist.validation.labels==2]\n",
    "valY_ones = mnist.validation.labels[mnist.validation.labels==2]\n",
    "valX_others = mnist.validation.images[mnist.validation.labels!=2]\n",
    "valY_others = mnist.validation.labels[mnist.validation.labels!=2]\n",
    "valY_ones = valY_ones[:,None]\n",
    "valY_others = valY_others[:,None]\n",
    "val_ones = np.concatenate((valX_ones,valY_ones),axis=1)\n",
    "val_others = np.concatenate((valX_others,valY_others),axis=1)\n",
    "np.random.shuffle(val_others)\n",
    "val_others137 = val_others[0:137,:]\n",
    "val = np.concatenate((val_ones,val_others137),axis=0)\n",
    "np.random.shuffle(val)\n",
    "valX = val[:,0:-1]\n",
    "valY = val[:,-1]\n",
    "valY[valY==0]=1\n",
    "valY[valY==2]=0\n",
    "valY[valY!=0]=1\n",
    "\n",
    "sum(valY)\n",
    "\n",
    "# Test data prepration\n",
    "\n",
    "np.random.seed(42)\n",
    "testX_ones = mnist.test.images[mnist.test.labels==2]\n",
    "testY_ones = mnist.test.labels[mnist.test.labels==2]\n",
    "testX_others = mnist.test.images[mnist.test.labels!=2]\n",
    "testY_others = mnist.test.labels[mnist.test.labels!=2]\n",
    "testY_ones = testY_ones[:,None]\n",
    "testY_others = testY_others[:,None]\n",
    "test_ones = np.concatenate((testX_ones,testY_ones),axis=1)\n",
    "test_others = np.concatenate((testX_others,testY_others),axis=1)\n",
    "np.random.shuffle(test_others)\n",
    "test_others137 = test_others[0:137,:]\n",
    "test = np.concatenate((test_ones,test_others137),axis=0)\n",
    "np.random.shuffle(test)\n",
    "testX = test[:,0:-1]\n",
    "testY = test[:,-1]\n",
    "testY[testY==0]=1\n",
    "testY[testY==2]=0\n",
    "testY[testY!=0]=1\n",
    "\n",
    "sum(testY)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data checking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 3600x216 with 11 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "n_samples = 25\n",
    "\n",
    "plt.figure(figsize=(n_samples * 2, 3))\n",
    "for index in range(14,n_samples):\n",
    "    plt.subplot(1, n_samples, index + 1)\n",
    "    sample_image = X_train[index].reshape(28, 28)\n",
    "    plt.imshow(sample_image, cmap=\"binary\")\n",
    "    plt.axis(\"off\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 1., 0., 0., 1., 0., 0.], dtype=float32)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_train[:n_samples]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparing Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-3-d9e33cb66a21>:32: calling reduce_sum (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "keep_dims is deprecated, use keepdims instead\n",
      "WARNING:tensorflow:From <ipython-input-3-d9e33cb66a21>:67: calling softmax (from tensorflow.python.ops.nn_ops) with dim is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "dim is deprecated, use axis instead\n"
     ]
    }
   ],
   "source": [
    "X = tf.placeholder(shape=[None, 28, 28, 1], dtype=tf.float32, name=\"X\")\n",
    "\n",
    "caps1_n_maps = 32\n",
    "caps1_n_caps = caps1_n_maps * 6 * 6  # 1152 primary capsules\n",
    "caps1_n_dims = 8\n",
    "\n",
    "conv1_params = {\n",
    "    \"filters\": 256,\n",
    "    \"kernel_size\": 9,\n",
    "    \"strides\": 1,\n",
    "    \"padding\": \"valid\",\n",
    "    \"activation\": tf.nn.relu,\n",
    "}\n",
    "\n",
    "conv2_params = {\n",
    "    \"filters\": caps1_n_maps * caps1_n_dims, # 256 convolutional filters\n",
    "    \"kernel_size\": 9,\n",
    "    \"strides\": 2,\n",
    "    \"padding\": \"valid\",\n",
    "    \"activation\": tf.nn.relu\n",
    "}\n",
    "\n",
    "conv1 = tf.layers.conv2d(X, name=\"conv1\", **conv1_params)\n",
    "conv2 = tf.layers.conv2d(conv1, name=\"conv2\", **conv2_params)\n",
    "\n",
    "caps1_raw = tf.reshape(conv2, [-1, caps1_n_caps, caps1_n_dims],\n",
    "                       name=\"caps1_raw\")\n",
    "\n",
    "def squash(s, axis=-1, epsilon=1e-7, name=None):\n",
    "    with tf.name_scope(name, default_name=\"squash\"):\n",
    "        squared_norm = tf.reduce_sum(tf.square(s), axis=axis,\n",
    "                                     keep_dims=True)\n",
    "        safe_norm = tf.sqrt(squared_norm + epsilon)\n",
    "        squash_factor = squared_norm / (1. + squared_norm)\n",
    "        unit_vector = s / safe_norm\n",
    "        return squash_factor * unit_vector\n",
    "\n",
    "caps1_output = squash(caps1_raw, name=\"caps1_output\")\n",
    "\n",
    "caps2_n_caps = 2\n",
    "caps2_n_dims = 16\n",
    "\n",
    "init_sigma = 0.1\n",
    "\n",
    "W_init = tf.random_normal(\n",
    "    shape=(1, caps1_n_caps, caps2_n_caps, caps2_n_dims, caps1_n_dims),\n",
    "    stddev=init_sigma, dtype=tf.float32, name=\"W_init\")\n",
    "W = tf.Variable(W_init, name=\"W\")\n",
    "\n",
    "batch_size = tf.shape(X)[0]\n",
    "W_tiled = tf.tile(W, [batch_size, 1, 1, 1, 1], name=\"W_tiled\")\n",
    "\n",
    "caps1_output_expanded = tf.expand_dims(caps1_output, -1,\n",
    "                                       name=\"caps1_output_expanded\")\n",
    "caps1_output_tile = tf.expand_dims(caps1_output_expanded, 2,\n",
    "                                   name=\"caps1_output_tile\")\n",
    "caps1_output_tiled = tf.tile(caps1_output_tile, [1, 1, caps2_n_caps, 1, 1],\n",
    "                             name=\"caps1_output_tiled\")\n",
    "\n",
    "caps2_predicted = tf.matmul(W_tiled, caps1_output_tiled,\n",
    "                            name=\"caps2_predicted\")\n",
    "\n",
    "# Dynamic Routing algorithm\n",
    "# Round 1\n",
    "raw_weights = tf.zeros([batch_size, caps1_n_caps, caps2_n_caps, 1, 1],\n",
    "                       dtype=np.float32, name=\"raw_weights\")\n",
    "routing_weights = tf.nn.softmax(raw_weights, dim=2, name=\"routing_weights\")\n",
    "\n",
    "weighted_predictions = tf.multiply(routing_weights, caps2_predicted,\n",
    "                                   name=\"weighted_predictions\")\n",
    "weighted_sum = tf.reduce_sum(weighted_predictions, axis=1, keep_dims=True,\n",
    "                             name=\"weighted_sum\")\n",
    "caps2_output_round_1 = squash(weighted_sum, axis=-2,\n",
    "                              name=\"caps2_output_round_1\")\n",
    "\n",
    "caps2_output_round_1_tiled = tf.tile(\n",
    "    caps2_output_round_1, [1, caps1_n_caps, 1, 1, 1],\n",
    "    name=\"caps2_output_round_1_tiled\")\n",
    "\n",
    "agreement1 = tf.matmul(caps2_predicted, caps2_output_round_1_tiled,\n",
    "                      transpose_a=True, name=\"agreement1\")\n",
    "# Round 2\n",
    "# Routing weight update\n",
    "raw_weights_round_2 = tf.add(raw_weights, agreement1,\n",
    "                             name=\"raw_weights_round_2\")\n",
    "routing_weights_round_2 = tf.nn.softmax(raw_weights_round_2,\n",
    "                                        dim=2,\n",
    "                                        name=\"routing_weights_round_2\")\n",
    "weighted_predictions_round_2 = tf.multiply(routing_weights_round_2,\n",
    "                                           caps2_predicted,\n",
    "                                           name=\"weighted_predictions_round_2\")\n",
    "weighted_sum_round_2 = tf.reduce_sum(weighted_predictions_round_2,\n",
    "                                     axis=1, keep_dims=True,\n",
    "                                     name=\"weighted_sum_round_2\")\n",
    "caps2_output_round_2 = squash(weighted_sum_round_2,\n",
    "                              axis=-2,\n",
    "                              name=\"caps2_output_round_2\")\n",
    "caps2_output_round_2_tiled = tf.tile(\n",
    "    caps2_output_round_2, [1, caps1_n_caps, 1, 1, 1],\n",
    "    name=\"caps2_output_round_2_tiled\")\n",
    "\n",
    "agreement2 = tf.matmul(caps2_predicted, caps2_output_round_2_tiled,\n",
    "                      transpose_a=True, name=\"agreement2\")\n",
    "\n",
    "# Round 3\n",
    "# Routing weight update\n",
    "raw_weights_round_3 = tf.add(raw_weights_round_2, agreement2,\n",
    "                             name=\"raw_weights_round_3\")\n",
    "routing_weights_round_3 = tf.nn.softmax(raw_weights_round_3,\n",
    "                                        dim=2,\n",
    "                                        name=\"routing_weights_round_3\")\n",
    "weighted_predictions_round_3 = tf.multiply(routing_weights_round_3,\n",
    "                                           caps2_predicted,\n",
    "                                           name=\"weighted_predictions_round_3\")\n",
    "weighted_sum_round_3 = tf.reduce_sum(weighted_predictions_round_3,\n",
    "                                     axis=1, keep_dims=True,\n",
    "                                     name=\"weighted_sum_round_3\")\n",
    "caps2_output_round_3 = squash(weighted_sum_round_3,\n",
    "                              axis=-2,\n",
    "                              name=\"caps2_output_round_3\")\n",
    "caps2_output_round_3_tiled = tf.tile(\n",
    "    caps2_output_round_3, [1, caps1_n_caps, 1, 1, 1],\n",
    "    name=\"caps2_output_round_3_tiled\")\n",
    "\n",
    "agreement3 = tf.matmul(caps2_predicted, caps2_output_round_3_tiled,\n",
    "                      transpose_a=True, name=\"agreement3\")\n",
    "\n",
    "# Round 4\n",
    "# Routing weight update\n",
    "raw_weights_round_4 = tf.add(raw_weights_round_3, agreement3,\n",
    "                             name=\"raw_weights_round_4\")\n",
    "routing_weights_round_4 = tf.nn.softmax(raw_weights_round_4,\n",
    "                                        dim=2,\n",
    "                                        name=\"routing_weights_round_4\")\n",
    "weighted_predictions_round_4 = tf.multiply(routing_weights_round_4,\n",
    "                                           caps2_predicted,\n",
    "                                           name=\"weighted_predictions_round_4\")\n",
    "weighted_sum_round_4 = tf.reduce_sum(weighted_predictions_round_4,\n",
    "                                     axis=1, keep_dims=True,\n",
    "                                     name=\"weighted_sum_round_4\")\n",
    "caps2_output_round_4 = squash(weighted_sum_round_4,\n",
    "                              axis=-2,\n",
    "                              name=\"caps2_output_round_4\")\n",
    "\"\"\"caps2_output_round_4_tiled = tf.tile(\n",
    "    caps2_output_round_4, [1, caps1_n_caps, 1, 1, 1],\n",
    "    name=\"caps2_output_round_4_tiled\")\n",
    "\n",
    "agreement4 = tf.matmul(caps2_predicted, caps2_output_round_4_tiled,\n",
    "                      transpose_a=True, name=\"agreement3\")\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "caps2_output = caps2_output_round_4\n",
    "\n",
    "def safe_norm(s, axis=-1, epsilon=1e-7, keep_dims=False, name=None):\n",
    "    with tf.name_scope(name, default_name=\"safe_norm\"):\n",
    "        squared_norm = tf.reduce_sum(tf.square(s), axis=axis,\n",
    "                                     keep_dims=keep_dims)\n",
    "        return tf.sqrt(squared_norm + epsilon)\n",
    "\n",
    "y_proba = safe_norm(caps2_output, axis=-2, name=\"y_proba\")\n",
    "\n",
    "\n",
    "y_proba_argmax = tf.argmax(y_proba, axis=2, name=\"y_proba\")\n",
    "\n",
    "y_pred = tf.squeeze(y_proba_argmax, axis=[1,2], name=\"y_pred\")\n",
    "\n",
    "y = tf.placeholder(shape=[None], dtype=tf.int64, name=\"y\")\n",
    "\n",
    "m_plus = 0.9\n",
    "m_minus = 0.1\n",
    "lambda_ = 0.5\n",
    "\n",
    "T = tf.one_hot(y, depth=caps2_n_caps, name=\"T\")\n",
    "\n",
    "caps2_output_norm = safe_norm(caps2_output, axis=-2, keep_dims=True,\n",
    "                              name=\"caps2_output_norm\")\n",
    "\n",
    "present_error_raw = tf.square(tf.maximum(0., m_plus - caps2_output_norm),\n",
    "                              name=\"present_error_raw\")\n",
    "present_error = tf.reshape(present_error_raw, shape=(-1, 2),\n",
    "                           name=\"present_error\")\n",
    "present_error\n",
    "\n",
    "absent_error_raw = tf.square(tf.maximum(0., caps2_output_norm - m_minus),\n",
    "                             name=\"absent_error_raw\")\n",
    "absent_error = tf.reshape(absent_error_raw, shape=(-1, 2),\n",
    "                          name=\"absent_error\")\n",
    "\n",
    "L = tf.add(T * present_error, lambda_ * (1.0 - T) * absent_error,\n",
    "           name=\"L\")\n",
    "\n",
    "margin_loss = tf.reduce_mean(tf.reduce_sum(L, axis=1), name=\"margin_loss\")\n",
    "\n",
    "mask_with_labels = tf.placeholder_with_default(False, shape=(),\n",
    "                                               name=\"mask_with_labels\")\n",
    "\n",
    "reconstruction_targets = tf.cond(mask_with_labels, # condition\n",
    "                                 lambda: y,        # if True\n",
    "                                 lambda: y_pred,   # if False\n",
    "                                 name=\"reconstruction_targets\")\n",
    "\n",
    "reconstruction_mask = tf.one_hot(reconstruction_targets,\n",
    "                                 depth=caps2_n_caps,\n",
    "                                 name=\"reconstruction_mask\")\n",
    "\n",
    "reconstruction_mask_reshaped = tf.reshape(\n",
    "    reconstruction_mask, [-1, 1, caps2_n_caps, 1, 1],\n",
    "    name=\"reconstruction_mask_reshaped\")\n",
    "\n",
    "caps2_output_masked = tf.multiply(\n",
    "    caps2_output, reconstruction_mask_reshaped,\n",
    "    name=\"caps2_output_masked\")\n",
    "\n",
    "decoder_input = tf.reshape(caps2_output_masked,\n",
    "                           [-1, caps2_n_caps * caps2_n_dims],\n",
    "                           name=\"decoder_input\")\n",
    "\n",
    "n_hidden1 = 512\n",
    "n_hidden2 = 1024\n",
    "n_output = 28 * 28\n",
    "\n",
    "with tf.name_scope(\"decoder\"):\n",
    "    hidden1 = tf.layers.dense(decoder_input, n_hidden1,\n",
    "                              activation=tf.nn.relu,\n",
    "                              name=\"hidden1\")\n",
    "    hidden2 = tf.layers.dense(hidden1, n_hidden2,\n",
    "                              activation=tf.nn.relu,\n",
    "                              name=\"hidden2\")\n",
    "    decoder_output = tf.layers.dense(hidden2, n_output,\n",
    "                                     activation=tf.nn.sigmoid,\n",
    "                                     name=\"decoder_output\")\n",
    "\n",
    "X_flat = tf.reshape(X, [-1, n_output], name=\"X_flat\")\n",
    "squared_difference = tf.square(X_flat - decoder_output,\n",
    "                               name=\"squared_difference\")\n",
    "reconstruction_loss = tf.reduce_mean(squared_difference,\n",
    "                                    name=\"reconstruction_loss\")\n",
    "\n",
    "alpha = 0.0005\n",
    "\n",
    "loss = tf.add(margin_loss, alpha * reconstruction_loss, name=\"loss\")\n",
    "\n",
    "\n",
    "correct = tf.equal(y, y_pred, name=\"correct\")\n",
    "accuracy = tf.reduce_mean(tf.cast(correct, tf.float32), name=\"accuracy\")\n",
    "\n",
    "optimizer = tf.train.AdamOptimizer()\n",
    "training_op = optimizer.minimize(loss, name=\"training_op\")\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1  Val accuracy: 92.8333%  Loss: 0.039372 (improved)\n",
      "Epoch: 2  Val accuracy: 97.5000%  Loss: 0.020195 (improved)\n",
      "Epoch: 3  Val accuracy: 98.5000%  Loss: 0.013374 (improved)\n",
      "Epoch: 4  Val accuracy: 99.5000%  Loss: 0.010962 (improved)\n",
      "Epoch: 5  Val accuracy: 99.0000%  Loss: 0.012394\n",
      "Epoch: 6  Val accuracy: 98.5000%  Loss: 0.018513\n",
      "Epoch: 7  Val accuracy: 98.5000%  Loss: 0.013231\n",
      "Epoch: 8  Val accuracy: 99.3333%  Loss: 0.009794 (improved)\n",
      "Epoch: 9  Val accuracy: 99.1667%  Loss: 0.009709 (improved)\n",
      "Epoch: 10  Val accuracy: 99.1667%  Loss: 0.009914\n"
     ]
    }
   ],
   "source": [
    "n_epochs = 10\n",
    "batch_size = 50\n",
    "restore_checkpoint = False\n",
    "n_iterations_per_epoch = len(X_train) // batch_size\n",
    "n_iterations_validation = len(valX) // batch_size\n",
    "best_loss_val = np.infty\n",
    "checkpoint_path = \"./my_capsule_network2\"\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    if restore_checkpoint and tf.train.checkpoint_exists(checkpoint_path):\n",
    "        saver.restore(sess, checkpoint_path)\n",
    "    else:\n",
    "        init.run()\n",
    "\n",
    "    for epoch in range(n_epochs):\n",
    "        for iteration in range(1, n_iterations_per_epoch + 1):\n",
    "            X_batch = X_train[(iteration-1)*batch_size:(iteration*batch_size),:]\n",
    "            y_batch = Y_train[(iteration-1)*batch_size:(iteration*batch_size)]\n",
    "            # Run the training operation and measure the loss:\n",
    "            _, loss_train = sess.run(\n",
    "                [training_op, loss],\n",
    "                feed_dict={X: X_batch.reshape([-1, 28, 28, 1]),\n",
    "                           y: y_batch,\n",
    "                           mask_with_labels: True})\n",
    "            print(\"\\rIteration: {}/{} ({:.1f}%)  Loss: {:.5f}\".format(\n",
    "                      iteration, n_iterations_per_epoch,\n",
    "                      iteration * 100 / n_iterations_per_epoch,\n",
    "                      loss_train),\n",
    "                  end=\"\")\n",
    "\n",
    "        # At the end of each epoch,  \n",
    "        # measure the validation loss and accuracy:\n",
    "        loss_vals = []\n",
    "        acc_vals = []\n",
    "        for iteration in range(1, n_iterations_validation + 1):\n",
    "            X_batch = valX[(iteration-1)*batch_size:(iteration*batch_size),:]\n",
    "            y_batch = valY[(iteration-1)*batch_size:(iteration*batch_size)]\n",
    "            loss_val, acc_val = sess.run(\n",
    "                    [loss, accuracy],\n",
    "                    feed_dict={X: X_batch.reshape([-1, 28, 28, 1]),\n",
    "                               y: y_batch})\n",
    "            loss_vals.append(loss_val)\n",
    "            acc_vals.append(acc_val)\n",
    "            print(\"\\rEvaluating the model: {}/{} ({:.1f}%)\".format(\n",
    "                      iteration, n_iterations_validation,\n",
    "                      iteration * 100 / n_iterations_validation),\n",
    "                  end=\" \" * 10)\n",
    "        loss_val = np.mean(loss_vals)\n",
    "        acc_val = np.mean(acc_vals)\n",
    "        print(\"\\rEpoch: {}  Val accuracy: {:.4f}%  Loss: {:.6f}{}\".format(\n",
    "            epoch + 1, acc_val * 100, loss_val,\n",
    "            \" (improved)\" if loss_val < best_loss_val else \"\"))\n",
    "\n",
    "        # And save the model if it improved: \n",
    "        \n",
    "        if loss_val < best_loss_val:\n",
    "            save_path = saver.save(sess, checkpoint_path)\n",
    "            best_loss_val = loss_val"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./my_capsule_network2\n",
      "Final test accuracy: 99.5652%  Loss: 0.004372 \n"
     ]
    }
   ],
   "source": [
    "n_iterations_test = len(testX) // batch_size\n",
    "checkpoint_path = \"./my_capsule_network2\"\n",
    "with tf.Session() as sess:\n",
    "    saver.restore(sess, checkpoint_path)\n",
    "    pred = []\n",
    "    loss_tests = []\n",
    "    acc_tests = []\n",
    "    for iteration in range(1, n_iterations_test + 1):\n",
    "        X_batch = testX[(iteration-1)*batch_size:(iteration*batch_size),:]\n",
    "        y_batch = testY[(iteration-1)*batch_size:(iteration*batch_size)]\n",
    "        loss_test, acc_test = sess.run(\n",
    "                [loss, accuracy],\n",
    "                feed_dict={X: X_batch.reshape([-1, 28, 28, 1]),\n",
    "                           y: y_batch})\n",
    "        loss_tests.append(loss_test)\n",
    "        pred.append(y_pred)\n",
    "        acc_tests.append(acc_test)\n",
    "        print(\"\\rEvaluating the model: {}/{} ({:.1f}%)\".format(\n",
    "                  iteration, n_iterations_test,\n",
    "                  iteration * 100 / n_iterations_test),\n",
    "              end=\" \" * 10)\n",
    "    loss_test = np.mean(loss_tests)\n",
    "    #print(tf.confusion_matrix())\n",
    "    acc_test = np.mean(acc_tests)\n",
    "    print(\"\\rFinal test accuracy: {:.4f}%  Loss: {:.6f}\".format(\n",
    "        acc_test * 100, loss_test))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predictions for training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./my_capsule_network2\n",
      "\n",
      "Accuracy is :  1.0\n",
      "\n",
      "AUROC is :  1.0\n",
      "\n",
      "Confusion Matrix is :\n",
      "[[5470    0]\n",
      " [   0  321]]\n",
      "\n",
      "Classification report is : \n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       1.00      1.00      1.00      5470\n",
      "        1.0       1.00      1.00      1.00       321\n",
      "\n",
      "avg / total       1.00      1.00      1.00      5791\n",
      "\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    saver.restore(sess, checkpoint_path)\n",
    "    y_pred_value = sess.run(\n",
    "            [y_pred],\n",
    "            feed_dict={X: X_train.reshape([-1, 28, 28, 1]),\n",
    "                       y: np.array([], dtype=np.int64)})\n",
    "pred = np.array(y_pred_value).T\n",
    "pred = pred.flatten()\n",
    "Y_train = Y_train.flatten()\n",
    "\n",
    "from sklearn.metrics import classification_report,confusion_matrix,roc_auc_score,accuracy_score\n",
    "print(\"\")\n",
    "print(\"Accuracy is : \", accuracy_score(Y_train,pred))\n",
    "print(\"\")\n",
    "print(\"AUROC is : \", roc_auc_score(Y_train,pred))\n",
    "print(\"\")\n",
    "print(\"Confusion Matrix is :\")\n",
    "print(confusion_matrix(Y_train, pred, labels=None, sample_weight=None))\n",
    "print(\"\")\n",
    "print(\"Classification report is : \")\n",
    "print(\"\")\n",
    "print(classification_report(Y_train, pred, labels=None, sample_weight=None))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predictions for validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./my_capsule_network2\n",
      "\n",
      "Accuracy is :  0.9904\n",
      "\n",
      "AUROC is :  0.9781021897810219\n",
      "\n",
      "Confusion Matrix is :\n",
      "[[488   0]\n",
      " [  6 131]]\n",
      "\n",
      "Classification report is : \n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.99      1.00      0.99       488\n",
      "        1.0       1.00      0.96      0.98       137\n",
      "\n",
      "avg / total       0.99      0.99      0.99       625\n",
      "\n"
     ]
    }
   ],
   "source": [
    "with tf.Session(config=tf.ConfigProto(\n",
    "      allow_soft_placement=True, log_device_placement=True)) as sess:\n",
    "    saver.restore(sess, checkpoint_path)\n",
    "    y_pred_value = sess.run(\n",
    "            [y_pred],\n",
    "            feed_dict={X: valX.reshape([-1, 28, 28, 1]),\n",
    "                       y: np.array([], dtype=np.int64)})\n",
    "pred = np.array(y_pred_value).T\n",
    "pred = pred.flatten()\n",
    "valY = valY.flatten()\n",
    "\n",
    "from sklearn.metrics import classification_report,confusion_matrix,roc_auc_score,accuracy_score\n",
    "print(\"\")\n",
    "print(\"Accuracy is : \", accuracy_score(valY,pred))\n",
    "print(\"\")\n",
    "print(\"AUROC is : \", roc_auc_score(valY,pred))\n",
    "print(\"\")\n",
    "print(\"Confusion Matrix is :\")\n",
    "print(confusion_matrix(valY, pred, labels=None, sample_weight=None))\n",
    "print(\"\")\n",
    "print(\"Classification report is : \")\n",
    "print(\"\")\n",
    "print(classification_report(valY, pred, labels=None, sample_weight=None))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test set prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./my_capsule_network2\n",
      "\n",
      "Accuracy is :  0.9957228400342173\n",
      "\n",
      "AUROC is :  0.9849169637299836\n",
      "\n",
      "Confusion Matrix is :\n",
      "[[1031    1]\n",
      " [   4  133]]\n",
      "\n",
      "Classification report is : \n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       1.00      1.00      1.00      1032\n",
      "        1.0       0.99      0.97      0.98       137\n",
      "\n",
      "avg / total       1.00      1.00      1.00      1169\n",
      "\n"
     ]
    }
   ],
   "source": [
    "with tf.Session(config=tf.ConfigProto(\n",
    "      allow_soft_placement=True, log_device_placement=True)) as sess:\n",
    "    saver.restore(sess, checkpoint_path)\n",
    "    y_pred_value = sess.run(\n",
    "            [y_pred],\n",
    "            feed_dict={X: testX.reshape([-1, 28, 28, 1]),\n",
    "                       y: np.array([], dtype=np.int64)})\n",
    "pred = np.array(y_pred_value).T\n",
    "pred = pred.flatten()\n",
    "testY = testY.flatten()\n",
    "\n",
    "from sklearn.metrics import classification_report,confusion_matrix,roc_auc_score,accuracy_score\n",
    "print(\"\")\n",
    "print(\"Accuracy is : \", accuracy_score(testY,pred))\n",
    "print(\"\")\n",
    "print(\"AUROC is : \", roc_auc_score(testY,pred))\n",
    "print(\"\")\n",
    "print(\"Confusion Matrix is :\")\n",
    "print(confusion_matrix(testY, pred, labels=None, sample_weight=None))\n",
    "print(\"\")\n",
    "print(\"Classification report is : \")\n",
    "print(\"\")\n",
    "print(classification_report(testY, pred, labels=None, sample_weight=None))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AUROC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./my_capsule_network2\n",
      "AUROC is :  0.999766593108131\n",
      "\n"
     ]
    }
   ],
   "source": [
    "checkpoint_path = \"./my_capsule_network2\"\n",
    "with tf.Session(config=tf.ConfigProto(\n",
    "      allow_soft_placement=True, log_device_placement=True)) as sess:\n",
    "    saver.restore(sess, checkpoint_path)\n",
    "    prob = sess.run(\n",
    "            [y_proba],\n",
    "            feed_dict={X: testX.reshape([-1, 28, 28, 1]),\n",
    "                       y: np.array([], dtype=np.int64)})\n",
    "prob = np.array(prob)\n",
    "probs = np.array(prob[0,:,0,1,0])\n",
    "probs = probs.flatten()\n",
    "testY = testY.flatten()\n",
    "\n",
    "from sklearn.metrics import classification_report,confusion_matrix,roc_auc_score,accuracy_score\n",
    "print(\"AUROC is : \", roc_auc_score(testY,probs))\n",
    "print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
